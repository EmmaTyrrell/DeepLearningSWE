{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ac281b-3e91-4fb8-a7c8-d63f4ba8d0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules established\n"
     ]
    }
   ],
   "source": [
    "# pseduo code\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "years = list(range(2024, 2025))\n",
    "Domain = \"Sierras\"\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "final_activation = 'relu'\n",
    "\n",
    "## seting folder\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# create folder for model outputs\n",
    "os.makedirs(modelOuptuts + f\"{str(timestamp)}/\", exist_ok=True)\n",
    "inter_model_outWorkspace = modelOuptuts + f\"{str(timestamp)}/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1940b914-a48b-4afa-8146-c9135f970ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for min-max scaling\n",
    "def min_max_scale(data, min_val=None, max_val=None, feature_range=(0, 1)):\n",
    "    \"\"\"Min-Max normalize a NumPy array to a target range.\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    mask = np.isnan(data)\n",
    "\n",
    "    d_min = np.nanmin(data) if min_val is None else min_val\n",
    "    d_max = np.nanmax(data) if max_val is None else max_val\n",
    "\n",
    "    # if d_max == d_min:\n",
    "    #     raise ValueError(\"Min and max are equal — can't scale.\")\n",
    "    if d_max == d_min:\n",
    "        return np.full_like(data, feature_range[0], dtype=np.float32)\n",
    "\n",
    "    a, b = feature_range\n",
    "    scaled = (data - d_min) / (d_max - d_min)  # to [0, 1]\n",
    "    scaled = scaled * (b - a) + a              # to [a, b]\n",
    "\n",
    "    scaled[mask] = np.nan  # preserve NaNs\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9dcff9b-f05b-4de4-a798-4abd5c94d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_aligned_raster(src_path, extent, target_shape, nodata_val=-1):\n",
    "    height, width = target_shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        try:\n",
    "            data = src.read(\n",
    "                1,\n",
    "                out_shape=target_shape,\n",
    "                resampling=rasterio.enums.Resampling.nearest,\n",
    "                window=src.window(*extent)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {src_path}: {e}\")\n",
    "            return np.full(target_shape, nodata_val, dtype=np.float32)\n",
    "\n",
    "        # Handle nodata in source\n",
    "        src_nodata = src.nodata\n",
    "        if src_nodata is not None:\n",
    "            data = np.where(data == src_nodata, np.nan, data)\n",
    "\n",
    "        # Replace NaNs or invalid with -1\n",
    "        data = np.where(np.isnan(data), nodata_val, data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46aa5377-0011-4db5-9dd1-b6f87e72fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_array_as_raster(output_path, array, extent, crs, nodata_val=-1):\n",
    "    height, width = array.shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=array.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "        nodata=nodata_val\n",
    "    ) as dst:\n",
    "        dst.write(array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e57c3d3-fd37-4067-a32b-4d53fb69db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2024\n",
      "You go girl!\n",
      "all data split into target and feature array\n"
     ]
    }
   ],
   "source": [
    "# split up the features and arrarys \n",
    "\n",
    "## create empty arrays\n",
    "featureArray = []\n",
    "targetArray = []\n",
    "\n",
    "# loop through the years and feature data\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}\")\n",
    "    targetSplits = WorkspaceBase + f\"{year}/SWE_processed_splits/\"\n",
    "    fSCAWorkspace = WorkspaceBase + f\"{year}/fSCA/\"\n",
    "    for sample in os.listdir(targetSplits):\n",
    "        featureTuple = ()\n",
    "        featureName = []\n",
    "        # loop through each sample and get the corresponding features\n",
    "        if sample.endswith(\"nonull_fnl.tif\"):\n",
    "            # read in data\n",
    "            with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                samp_data = samp_src.read(1)\n",
    "                meta = samp_src.meta.copy()\n",
    "                samp_extent = samp_src.bounds\n",
    "                samp_transform = samp_src.transform\n",
    "                samp_crs = samp_src.crs\n",
    "    \n",
    "                # apply a mask to all no data values. Reminder that nodata values is -9999\n",
    "                mask = samp_data >= 0\n",
    "                msked_target = np.where(mask, samp_data, -1)\n",
    "                target_shape = msked_target.shape\n",
    "    \n",
    "                # flatted data\n",
    "                samp_flat = msked_target.flatten()\n",
    "                \n",
    "\n",
    "            # try to get the fsca variables \n",
    "            sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "            for fSCA in os.listdir(fSCAWorkspace):\n",
    "                if fSCA.endswith(\".tif\") and fSCA.startswith(sample_root):\n",
    "                    featureName.append(f\"{fSCA[:-4]}\")\n",
    "                    fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                    fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                    featureTuple += (fsca_norm,)\n",
    "                    # print(fsca_norm.shape)\n",
    "                    if fsca_norm.shape != (256, 256):\n",
    "                        print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                        output_debug_path = f\"./debug_output/{sample_root}_BAD_FSCA.tif\"\n",
    "                        save_array_as_raster(\n",
    "                            output_path=output_debug_path,\n",
    "                            array=fsca_norm,\n",
    "                            extent=samp_extent,\n",
    "                            crs=samp_crs,\n",
    "                            nodata_val=-1\n",
    "                        )\n",
    "    \n",
    "            # get a DOY array into a feature \n",
    "            date_string = sample.split(\"_\")[1]\n",
    "            doy_str = date_string[-3:]\n",
    "            doy = float(doy_str)\n",
    "            DOY_array = np.full_like(msked_target, doy)\n",
    "            doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "            featureTuple += (doy_norm,)\n",
    "            featureName.append(doy)\n",
    "    \n",
    "            # get the vegetation array\n",
    "            for tree in os.listdir(tree_workspace):\n",
    "                if tree.endswith(\".tif\"):\n",
    "                    if tree.startswith(f\"{year}\"):\n",
    "                        featureName.append(f\"{tree[:-4]}\")\n",
    "                        tree_norm = read_aligned_raster(\n",
    "                        src_path=tree_workspace + tree,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (tree_norm,)\n",
    "                        if tree_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                            output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            \n",
    "            # # get all the features in the fodler \n",
    "            for phv in os.listdir(phv_features):\n",
    "                if phv.endswith(\".tif\"):\n",
    "                    featureName.append(f\"{phv[:-4]}\")\n",
    "                    phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                    featureTuple += (phv_data,)\n",
    "                    if phv_data.shape != (256, 256):\n",
    "                         print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                        \n",
    "            feature_stack = np.dstack(featureTuple)\n",
    "            if feature_stack.shape[2] != 14:\n",
    "                print(f\"⚠️ {sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                print(featureName)\n",
    "                print(\" \")\n",
    "            else:\n",
    "                featureArray.append(feature_stack)\n",
    "                targetArray.append(samp_flat)\n",
    "    print(\"You go girl!\")\n",
    "X = np.array(featureArray)\n",
    "y = np.array(targetArray)\n",
    "print(\"all data split into target and feature array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4f139f-1221-481a-94c2-d97dbc2ad56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data\n",
      "feature shape: (1106, 256, 256, 14)\n",
      "target shape: (1106, 65536)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of input data\")\n",
    "print(f\"feature shape: {X.shape}\")\n",
    "print(f\"target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580cd460-4fee-447c-950d-718f514018c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "________________________________ Training and Validation Data Shapes ________________________________\n",
      "Training data shape: (940, 256, 256, 14) (940, 65536)\n",
      "Validation data shape: (166, 256, 256, 14) (166, 65536)\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "# split between training and test data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "print(\" \")\n",
    "print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a2dc07-a97b-4965-9e2f-7f30270beb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(256, 256, 14), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "# add stacked convolutions\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# dense layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "# output layer\n",
    "model.add(Dense(65536, activation=final_activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71f8e2b5-450b-41fa-9f2c-54bb6c2c449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@register_keras_serializable()\n",
    "def masked_loss_fn(y_true, y_pred, loss_fn, mask_value=-1):\n",
    "    mask = tf.not_equal(y_true, mask_value)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return loss_fn(y_true_masked, y_pred_masked)\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mse\")\n",
    "def masked_mse(y_true, y_pred):\n",
    "    return masked_loss_fn(y_true, y_pred, MeanSquaredError())\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mae\")\n",
    "def masked_mae(y_true, y_pred):\n",
    "    return masked_loss_fn(y_true, y_pred, MeanAbsoluteError())\n",
    "\n",
    "@register_keras_serializable(name=\"masked_rmse\")\n",
    "def masked_rmse(y_true, y_pred):\n",
    "    mse = masked_loss_fn(y_true, y_pred, MeanSquaredError())\n",
    "    return tf.sqrt(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57fa7116-6318-42c9-a600-f0bce472d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Fetch batch data\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "\n",
    "        # Optionally, do preprocessing or masking here\n",
    "        # (e.g., ensure y_batch is ready for custom loss with -1 masking)\n",
    "        y_batch[y_batch == -1] = 0\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e7d4ab-d763-47c5-8ffd-6f8f37953ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 64)      8128      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 254, 254, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 125, 125, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              25691136  \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 65536)             67174400  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,248,576\n",
      "Trainable params: 93,245,632\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# uploading and saving model stats\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", monitor=\"val_masked_rmse\",\n",
    "    verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_masked_rmse\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "#compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=masked_mse,  # function version, already mask-aware\n",
    "    metrics=[masked_rmse, masked_mae, masked_mse]\n",
    ")\n",
    "\n",
    "# get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb25ac2-5780-4744-b7c4-b1e74f37ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0544 - masked_rmse: 0.2330 - masked_mae: 0.1132 - masked_mse: 0.0557  \n",
      "Epoch 1: val_masked_rmse improved from inf to 0.26334, saving model to D:/ASOML/Sierras/modelOutputs/20250406_172455\\best_model_20250406_172455.keras\n",
      "30/30 [==============================] - 234s 8s/step - loss: 0.0544 - masked_rmse: 0.2330 - masked_mae: 0.1132 - masked_mse: 0.0557 - val_loss: 0.0675 - val_masked_rmse: 0.2633 - val_masked_mae: 0.1034 - val_masked_mse: 0.0697\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0406 - masked_rmse: 0.2000 - masked_mae: 0.0908 - masked_mse: 0.0405  \n",
      "Epoch 2: val_masked_rmse improved from 0.26334 to 0.25334, saving model to D:/ASOML/Sierras/modelOutputs/20250406_172455\\best_model_20250406_172455.keras\n",
      "30/30 [==============================] - 224s 8s/step - loss: 0.0406 - masked_rmse: 0.2000 - masked_mae: 0.0908 - masked_mse: 0.0405 - val_loss: 0.0676 - val_masked_rmse: 0.2533 - val_masked_mae: 0.1012 - val_masked_mse: 0.0654\n",
      "Epoch 3/100\n",
      " 1/30 [>.............................] - ETA: 1:44 - loss: 0.0296 - masked_rmse: 0.1721 - masked_mae: 0.0760 - masked_mse: 0.0296"
     ]
    }
   ],
   "source": [
    "# establish the model\n",
    "batch_size = 32\n",
    "train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33daad-ddc0-4900-a149-78481ee6aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# 1. Masked MSE (used as loss)\n",
    "axs[0].plot(history.history['loss'], label='Train MSE Loss')\n",
    "axs[0].plot(history.history['val_loss'], label='Val MSE Loss')\n",
    "axs[0].set_ylabel('MSE Loss')\n",
    "axs[0].set_title('Masked MSE')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# 2. Masked RMSE\n",
    "axs[1].plot(history.history['masked_rmse'], label='Train RMSE')\n",
    "axs[1].plot(history.history['val_masked_rmse'], label='Val RMSE')\n",
    "axs[1].set_ylabel('RMSE')\n",
    "axs[1].set_title('Masked RMSE')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# 3. Masked MAE\n",
    "axs[2].plot(history.history['masked_mae'], label='Train MAE')\n",
    "axs[2].plot(history.history['val_masked_mae'], label='Val MAE')\n",
    "axs[2].set_ylabel('MAE')\n",
    "axs[2].set_title('Masked MAE')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].legend()\n",
    "axs[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(inter_model_outWorkspace + \"Model_error_epochs.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cf1c1-e07d-4343-b27d-736766475e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model.fit() has completed\n",
    "f = open(inter_model_outWorkspace + \"output.txt\", \"a\")\n",
    "sys.stdout = f\n",
    "metrics_to_track = ['val_masked_rmse', 'val_masked_mse', 'val_masked_mae']\n",
    "\n",
    "print(\"\\n📊 Validation Metric Progression:\")\n",
    "for metric in metrics_to_track:\n",
    "    values = history.history.get(metric, [])\n",
    "    if values:\n",
    "        print(f\"{metric}: Start = {values[0]:.4f}, End = {values[-1]:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: Not found in history.\")\n",
    "print(f\"Final activation function: {final_activation}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e27f96-8b01-4128-be68-baa4fa2460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks(start_year, end_year, target_splits_path, fSCA_path, vegetation_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        years = list(range(start_year, (int(end_year) + 1)))\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        for year in years:\n",
    "            print(f\"Processing {year}\")\n",
    "            targetSplits = target_splits_path\n",
    "            fSCAWorkspace = fSCA_path\n",
    "            for sample in os.listdir(targetSplits):\n",
    "                featureTuple = ()\n",
    "                featureName = []\n",
    "                # loop through each sample and get the corresponding features\n",
    "                if sample.endswith(extension_filter):\n",
    "                    # read in data\n",
    "                    with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                        samp_data = samp_src.read(1)\n",
    "                        meta = samp_src.meta.copy()\n",
    "                        samp_extent = samp_src.bounds\n",
    "                        samp_transform = samp_src.transform\n",
    "                        samp_crs = samp_src.crs\n",
    "                        # apply a no-data mask\n",
    "                        mask = samp_data >= 0\n",
    "                        msked_target = np.where(mask, samp_data, -1)\n",
    "                        target_shape = msked_target.shape\n",
    "            \n",
    "                        # flatted data\n",
    "                        samp_flat = msked_target.flatten()\n",
    "                        \n",
    "        \n",
    "                    # try to get the fsca variables \n",
    "                    sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                    for fSCA in os.listdir(fSCAWorkspace):\n",
    "                        if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                            featureName.append(f\"{fSCA[:-4]}\")\n",
    "                            fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                            fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                            featureTuple += (fsca_norm,)\n",
    "                            # print(fsca_norm.shape)\n",
    "                            if fsca_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "            \n",
    "                    # get a DOY array into a feature \n",
    "                    date_string = sample.split(\"_\")[1]\n",
    "                    doy_str = date_string[-3:]\n",
    "                    doy = float(doy_str)\n",
    "                    DOY_array = np.full_like(msked_target, doy)\n",
    "                    doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                    featureTuple += (doy_norm,)\n",
    "                    featureName.append(doy)\n",
    "            \n",
    "                    # get the vegetation array\n",
    "                    for tree in os.listdir(vegetation_path):\n",
    "                        if tree.endswith(extension_filter):\n",
    "                            if tree.startswith(f\"{year}\"):\n",
    "                                featureName.append(f\"{tree[:-4]}\")\n",
    "                                tree_norm = read_aligned_raster(\n",
    "                                src_path=tree_workspace + tree,\n",
    "                                extent=samp_extent,\n",
    "                                target_shape=target_shape\n",
    "                                )\n",
    "                                tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                                featureTuple += (tree_norm,)\n",
    "                                if tree_norm.shape != desired_shape:\n",
    "                                    print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                    output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                    save_array_as_raster(\n",
    "                                        output_path=output_debug_path,\n",
    "                                        array=fsca_norm,\n",
    "                                        extent=samp_extent,\n",
    "                                        crs=samp_crs,\n",
    "                                        nodata_val=-1\n",
    "                                    )\n",
    "                    \n",
    "            \n",
    "                    # # get all the features in the fodler \n",
    "                    for phv in os.listdir(phv_path):\n",
    "                        if phv.endswith(extension_filter):\n",
    "                            featureName.append(f\"{phv[:-4]}\")\n",
    "                            phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                            featureTuple += (phv_data,)\n",
    "                            if phv_data.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                    feature_stack = np.dstack(featureTuple)\n",
    "                    if feature_stack.shape[2] != num_of_channels:\n",
    "                        print(f\"⚠️ {sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                        print(featureName)\n",
    "                        print(\" \")\n",
    "                    else:\n",
    "                        featureArray.append(feature_stack)\n",
    "                        targetArray.append(samp_flat)\n",
    "                        extent_list.append(samp_extent)\n",
    "                        crs_list.append(samp_crs)\n",
    "            print(\"You go girl!\")\n",
    "        print(\"all data split into target and feature array\")\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1b35e-4777-4532-a516-3022e3a440e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks_testGroups(year, target_splits_path, fSCA_path, vegetation_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        # print(f\"Processing {group}\")\n",
    "        targetSplits = target_splits_path\n",
    "        fSCAWorkspace = fSCA_path\n",
    "        for sample in os.listdir(targetSplits):\n",
    "            featureTuple = ()\n",
    "            featureName = []\n",
    "            # loop through each sample and get the corresponding features\n",
    "            if sample.endswith(extension_filter):\n",
    "                # read in data\n",
    "                with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                    samp_data = samp_src.read(1)\n",
    "                    meta = samp_src.meta.copy()\n",
    "                    samp_extent = samp_src.bounds\n",
    "                    samp_transform = samp_src.transform\n",
    "                    samp_crs = samp_src.crs\n",
    "                    # apply a no-data mask\n",
    "                    mask = samp_data >= 0\n",
    "                    msked_target = np.where(mask, samp_data, -1)\n",
    "                    target_shape = msked_target.shape\n",
    "        \n",
    "                    # flatted data\n",
    "                    samp_flat = msked_target.flatten()\n",
    "                    \n",
    "    \n",
    "                # try to get the fsca variables \n",
    "                sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                for fSCA in os.listdir(fSCAWorkspace):\n",
    "                    if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                        featureName.append(f\"{fSCA[:-4]}\")\n",
    "                        fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                        fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (fsca_norm,)\n",
    "                        # print(fsca_norm.shape)\n",
    "                        if fsca_norm.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "        \n",
    "                # get a DOY array into a feature \n",
    "                date_string = sample.split(\"_\")[1]\n",
    "                doy_str = date_string[-3:]\n",
    "                doy = float(doy_str)\n",
    "                DOY_array = np.full_like(msked_target, doy)\n",
    "                doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                featureTuple += (doy_norm,)\n",
    "                featureName.append(doy)\n",
    "        \n",
    "                # get the vegetation array\n",
    "                for tree in os.listdir(vegetation_path):\n",
    "                    if tree.endswith(extension_filter):\n",
    "                        if tree.startswith(f\"{year}\"):\n",
    "                            featureName.append(f\"{tree[:-4]}\")\n",
    "                            tree_norm = read_aligned_raster(\n",
    "                            src_path=tree_workspace + tree,\n",
    "                            extent=samp_extent,\n",
    "                            target_shape=target_shape\n",
    "                            )\n",
    "                            tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                            featureTuple += (tree_norm,)\n",
    "                            if tree_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                \n",
    "        \n",
    "                # # get all the features in the fodler \n",
    "                for phv in os.listdir(phv_path):\n",
    "                    if phv.endswith(extension_filter):\n",
    "                        featureName.append(f\"{phv[:-4]}\")\n",
    "                        phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                        featureTuple += (phv_data,)\n",
    "                        if phv_data.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "                feature_stack = np.dstack(featureTuple)\n",
    "                if feature_stack.shape[2] != num_of_channels:\n",
    "                    print(f\"⚠️ {sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                    print(featureName)\n",
    "                    print(\" \")\n",
    "                else:\n",
    "                    featureArray.append(feature_stack)\n",
    "                    targetArray.append(samp_flat)\n",
    "                    extent_list.append(samp_extent)\n",
    "                    crs_list.append(samp_crs)\n",
    "        # print(\"You go girl!\")\n",
    "        # print(\"all data split into target and feature array\")\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac44b85-6468-41d3-a8ab-f64129c3fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"Testing Group 1\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup1 = inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\"\n",
    "X_train_g1, y_train_g1, g1_train_extents, g1_train_crs = target_feature_stacks_testGroups(year=2022, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group1/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g1, y_test_g1, g1_test_extents, g1_test_crs = target_feature_stacks_testGroups(year = 2022,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group1/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g1 = np.array(X_test_g1)\n",
    "y_test_array_g1 = np.array(y_test_g1)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g1.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g1.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g1 = model.predict(X_test_array_g1, batch_size=32)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g1, y_test_array_g1, batch_size=32)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g1):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g1[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup1}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g1_test_extents[i],\n",
    "        crs=g1_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g1, y_test_array_g1, batch_size=32, return_dict=True)\n",
    "print(\"\\n📊 Test Group 1 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef485842-458d-4759-bc12-19eb6398356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"\\n Testing Group 2\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G2_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup2 = inter_model_outWorkspace + f\"outTifs_G2_yPreds_tifs/\"\n",
    "\n",
    "X_train_g2, y_train_g2, g2_train_extents, g2_train_crs = target_feature_stacks_testGroups(year=2023, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group2/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2023/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g2, y_test_g2, g2_test_extents, g2_test_crs = target_feature_stacks_testGroups(year = 2023,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group2/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2023/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g2 = np.array(X_test_g2)\n",
    "y_test_array_g2 = np.array(y_test_g2)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g2.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g2.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g2 = model.predict(X_test_array_g2, batch_size=32)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g2, y_test_array_g2, batch_size=32)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g2):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g2[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup2}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g2_test_extents[i],\n",
    "        crs=g2_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g2, y_test_array_g2, batch_size=32, return_dict=True)\n",
    "print(\"\\n📊 Test Group 2 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973c25e-1351-4229-99b9-b37c404855d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"\\n Testing Group 3\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G3_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup3 = inter_model_outWorkspace + f\"outTifs_G3_yPreds_tifs/\"\n",
    "\n",
    "X_train_g3, y_train_g3, g3_train_extents, g3_train_crs = target_feature_stacks_testGroups(year=2024, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group3/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2024/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g3, y_test_g3, g3_test_extents, g3_test_crs = target_feature_stacks_testGroups(year = 2024,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group3/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2024/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g3 = np.array(X_test_g3)\n",
    "y_test_array_g3 = np.array(y_test_g3)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g3.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g3.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g3 = model.predict(X_test_array_g3, batch_size=32)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g3, y_test_array_g3, batch_size=32)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g3):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g3[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup3}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g3_test_extents[i],\n",
    "        crs=g3_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g3, y_test_array_g3, batch_size=32, return_dict=True)\n",
    "\n",
    "print(\"\\n📊 Test Group 3 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9e354-43d8-49a5-8efe-691ae5f798b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_seconds = end_time - start_time\n",
    "total_minutes = total_seconds / 60\n",
    "print(f\"Training time: {total_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac6d8a-eb1d-4b7a-bb2b-cbf7e27fa823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
