{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac281b-3e91-4fb8-a7c8-d63f4ba8d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseduo code\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "years = list(range(2022, 2023))\n",
    "Domain = \"Sierras\"\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940b914-a48b-4afa-8146-c9135f970ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for min-max scaling\n",
    "def min_max_scale(data, min_val=None, max_val=None, feature_range=(0, 1)):\n",
    "    \"\"\"Min-Max normalize a NumPy array to a target range.\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    mask = np.isnan(data)\n",
    "\n",
    "    d_min = np.nanmin(data) if min_val is None else min_val\n",
    "    d_max = np.nanmax(data) if max_val is None else max_val\n",
    "\n",
    "    # if d_max == d_min:\n",
    "    #     raise ValueError(\"Min and max are equal — can't scale.\")\n",
    "    if d_max == d_min:\n",
    "        return np.full_like(data, feature_range[0], dtype=np.float32)\n",
    "\n",
    "    a, b = feature_range\n",
    "    scaled = (data - d_min) / (d_max - d_min)  # to [0, 1]\n",
    "    scaled = scaled * (b - a) + a              # to [a, b]\n",
    "\n",
    "    scaled[mask] = np.nan  # preserve NaNs\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcff9b-f05b-4de4-a798-4abd5c94d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import from_bounds\n",
    "\n",
    "def read_aligned_raster(src_path, extent, target_shape, nodata_val=-1):\n",
    "    height, width = target_shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        try:\n",
    "            data = src.read(\n",
    "                1,\n",
    "                out_shape=target_shape,\n",
    "                resampling=rasterio.enums.Resampling.nearest,\n",
    "                window=src.window(*extent)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {src_path}: {e}\")\n",
    "            return np.full(target_shape, nodata_val, dtype=np.float32)\n",
    "\n",
    "        # Handle nodata in source\n",
    "        src_nodata = src.nodata\n",
    "        if src_nodata is not None:\n",
    "            data = np.where(data == src_nodata, np.nan, data)\n",
    "\n",
    "        # Replace NaNs or invalid with -1\n",
    "        data = np.where(np.isnan(data), nodata_val, data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa5377-0011-4db5-9dd1-b6f87e72fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_array_as_raster(output_path, array, extent, crs, nodata_val=-1):\n",
    "    height, width = array.shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=array.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "        nodata=nodata_val\n",
    "    ) as dst:\n",
    "        dst.write(array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57c3d3-fd37-4067-a32b-4d53fb69db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the features and arrarys \n",
    "\n",
    "## create empty arrays\n",
    "featureArray = []\n",
    "targetArray = []\n",
    "\n",
    "# loop through the years and feature data\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}\")\n",
    "    targetSplits = WorkspaceBase + f\"{year}/SWE_processed_splits/\"\n",
    "    fSCAWorkspace = WorkspaceBase + f\"{year}/fSCA/\"\n",
    "    for sample in os.listdir(targetSplits):\n",
    "        featureTuple = ()\n",
    "        featureName = []\n",
    "        # loop through each sample and get the corresponding features\n",
    "        if sample.endswith(\"nonull_fnl.tif\"):\n",
    "            # read in data\n",
    "            with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                samp_data = samp_src.read(1)\n",
    "                meta = samp_src.meta.copy()\n",
    "                samp_extent = samp_src.bounds\n",
    "                samp_transform = samp_src.transform\n",
    "                samp_crs = samp_src.crs\n",
    "    \n",
    "                # apply a mask to all no data values. Reminder that nodata values is -9999\n",
    "                mask = samp_data >= 0\n",
    "                msked_target = np.where(mask, samp_data, -1)\n",
    "                target_shape = msked_target.shape\n",
    "    \n",
    "                # flatted data\n",
    "                samp_flat = msked_target.flatten()\n",
    "                \n",
    "\n",
    "            # try to get the fsca variables \n",
    "            sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "            for fSCA in os.listdir(fSCAWorkspace):\n",
    "                if fSCA.endswith(\".tif\") and fSCA.startswith(sample_root):\n",
    "                    featureName.append(f\"{fSCA[:-4]}\")\n",
    "                    fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                    fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                    featureTuple += (fsca_norm,)\n",
    "                    # print(fsca_norm.shape)\n",
    "                    if fsca_norm.shape != (256, 256):\n",
    "                        print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                        output_debug_path = f\"./debug_output/{sample_root}_BAD_FSCA.tif\"\n",
    "                        save_array_as_raster(\n",
    "                            output_path=output_debug_path,\n",
    "                            array=fsca_norm,\n",
    "                            extent=samp_extent,\n",
    "                            crs=samp_crs,\n",
    "                            nodata_val=-1\n",
    "                        )\n",
    "    \n",
    "            # get a DOY array into a feature \n",
    "            date_string = sample.split(\"_\")[1]\n",
    "            doy_str = date_string[-3:]\n",
    "            doy = float(doy_str)\n",
    "            DOY_array = np.full_like(msked_target, doy)\n",
    "            doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "            featureTuple += (doy_norm,)\n",
    "            featureName.append(doy)\n",
    "    \n",
    "            # get the vegetation array\n",
    "            for tree in os.listdir(tree_workspace):\n",
    "                if tree.endswith(\".tif\"):\n",
    "                    if tree.startswith(f\"{year}\"):\n",
    "                        featureName.append(f\"{tree[:-4]}\")\n",
    "                        tree_norm = read_aligned_raster(\n",
    "                        src_path=tree_workspace + tree,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (tree_norm,)\n",
    "                        if tree_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                            output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            \n",
    "    \n",
    "            # # get all the features in the fodler \n",
    "            for phv in os.listdir(phv_features):\n",
    "                if phv.endswith(\".tif\"):\n",
    "                    featureName.append(f\"{phv[:-4]}\")\n",
    "                    phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                    featureTuple += (phv_data,)\n",
    "                    if phv_data.shape != (256, 256):\n",
    "                         print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                        \n",
    "            feature_stack = np.dstack(featureTuple)\n",
    "            if feature_stack.shape[2] != 14:\n",
    "                print(f\"⚠️ {sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                print(featureName)\n",
    "                print(\" \")\n",
    "            else:\n",
    "                featureArray.append(feature_stack)\n",
    "                targetArray.append(samp_flat)\n",
    "    print(\"You go girl!\")\n",
    "X = np.array(featureArray)\n",
    "y = np.array(targetArray)\n",
    "print(\"all data split into target and feature array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f139f-1221-481a-94c2-d97dbc2ad56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of input data\")\n",
    "print(f\"feature shape: {X.shape}\")\n",
    "print(f\"target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580cd460-4fee-447c-950d-718f514018c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split between training and test data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "print(\" \")\n",
    "print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2dc07-a97b-4965-9e2f-7f30270beb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(256, 256, 14), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "# add stacked convolutions\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# dense layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "# output layer\n",
    "model.add(Dense(65536, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3571fc1-704a-47d9-988a-169dfe11fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def masked_mse_loss(no_data_value=-1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Create mask where y_true != no_data_value\n",
    "        mask = tf.not_equal(y_true, no_data_value)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "        # Apply mask\n",
    "        squared_diff = tf.square(y_true - y_pred) * mask\n",
    "\n",
    "        # Avoid division by zero\n",
    "        loss_value = tf.reduce_sum(squared_diff) / tf.reduce_sum(mask)\n",
    "        return loss_value\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa7116-6318-42c9-a600-f0bce472d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Fetch batch data\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "\n",
    "        # Optionally, do preprocessing or masking here\n",
    "        # (e.g., ensure y_batch is ready for custom loss with -1 masking)\n",
    "        y_batch[y_batch == -1] = 0\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7d4ab-d763-47c5-8ffd-6f8f37953ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build in early stops\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"./best_model_{timestamp}.keras\", monitor=\"val_loss\",\n",
    "    verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss=masked_mse_loss(no_data_value=-1))\n",
    "\n",
    "# get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb25ac2-5780-4744-b7c4-b1e74f37ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish the model\n",
    "batch_size = 32\n",
    "train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "# model git\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33daad-ddc0-4900-a149-78481ee6aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Masked MSE Loss')\n",
    "plt.title('Training and Validation Masked MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e27f96-8b01-4128-be68-baa4fa2460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks(start_group, end_group, year, target_splits_path, fSCA_path, vegetation_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        groups = list(range(start_group, (int(end_group) + 1)))\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        for group in groups:\n",
    "            print(f\"Processing {group}\")\n",
    "            targetSplits = target_splits_path\n",
    "            fSCAWorkspace = fSCA_path\n",
    "            for sample in os.listdir(targetSplits):\n",
    "                featureTuple = ()\n",
    "                featureName = []\n",
    "                # loop through each sample and get the corresponding features\n",
    "                if sample.endswith(extension_filter):\n",
    "                    # read in data\n",
    "                    with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                        samp_data = samp_src.read(1)\n",
    "                        meta = samp_src.meta.copy()\n",
    "                        samp_extent = samp_src.bounds\n",
    "                        samp_transform = samp_src.transform\n",
    "                        samp_crs = samp_src.crs\n",
    "                        # apply a no-data mask\n",
    "                        mask = samp_data >= 0\n",
    "                        msked_target = np.where(mask, samp_data, -1)\n",
    "                        target_shape = msked_target.shape\n",
    "            \n",
    "                        # flatted data\n",
    "                        samp_flat = msked_target.flatten()\n",
    "                        \n",
    "        \n",
    "                    # try to get the fsca variables \n",
    "                    sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                    for fSCA in os.listdir(fSCAWorkspace):\n",
    "                        if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                            featureName.append(f\"{fSCA[:-4]}\")\n",
    "                            fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                            fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                            featureTuple += (fsca_norm,)\n",
    "                            # print(fsca_norm.shape)\n",
    "                            if fsca_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "            \n",
    "                    # get a DOY array into a feature \n",
    "                    date_string = sample.split(\"_\")[1]\n",
    "                    doy_str = date_string[-3:]\n",
    "                    doy = float(doy_str)\n",
    "                    DOY_array = np.full_like(msked_target, doy)\n",
    "                    doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                    featureTuple += (doy_norm,)\n",
    "                    featureName.append(doy)\n",
    "            \n",
    "                    # get the vegetation array\n",
    "                    for tree in os.listdir(vegetation_path):\n",
    "                        if tree.endswith(extension_filter):\n",
    "                            if tree.startswith(f\"{year}\"):\n",
    "                                featureName.append(f\"{tree[:-4]}\")\n",
    "                                tree_norm = read_aligned_raster(\n",
    "                                src_path=tree_workspace + tree,\n",
    "                                extent=samp_extent,\n",
    "                                target_shape=target_shape\n",
    "                                )\n",
    "                                tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                                featureTuple += (tree_norm,)\n",
    "                                if tree_norm.shape != desired_shape:\n",
    "                                    print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                    output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                    save_array_as_raster(\n",
    "                                        output_path=output_debug_path,\n",
    "                                        array=fsca_norm,\n",
    "                                        extent=samp_extent,\n",
    "                                        crs=samp_crs,\n",
    "                                        nodata_val=-1\n",
    "                                    )\n",
    "                    \n",
    "            \n",
    "                    # # get all the features in the fodler \n",
    "                    for phv in os.listdir(phv_path):\n",
    "                        if phv.endswith(extension_filter):\n",
    "                            featureName.append(f\"{phv[:-4]}\")\n",
    "                            phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                            featureTuple += (phv_data,)\n",
    "                            if phv_data.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                    feature_stack = np.dstack(featureTuple)\n",
    "                    if feature_stack.shape[2] != num_of_channels:\n",
    "                        print(f\"⚠️ {sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                        print(featureName)\n",
    "                        print(\" \")\n",
    "                    else:\n",
    "                        featureArray.append(feature_stack)\n",
    "                        targetArray.append(samp_flat)\n",
    "                        extent_list.append(samp_extent)\n",
    "                        crs_list.append(samp_crs)\n",
    "            print(\"You go girl!\")\n",
    "        print(\"all data split into target and feature array\")\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1b35e-4777-4532-a516-3022e3a440e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks_testGroups(year, target_splits_path, fSCA_path, vegetation_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        # print(f\"Processing {group}\")\n",
    "        targetSplits = target_splits_path\n",
    "        fSCAWorkspace = fSCA_path\n",
    "        for sample in os.listdir(targetSplits):\n",
    "            featureTuple = ()\n",
    "            featureName = []\n",
    "            # loop through each sample and get the corresponding features\n",
    "            if sample.endswith(extension_filter):\n",
    "                # read in data\n",
    "                with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                    samp_data = samp_src.read(1)\n",
    "                    meta = samp_src.meta.copy()\n",
    "                    samp_extent = samp_src.bounds\n",
    "                    samp_transform = samp_src.transform\n",
    "                    samp_crs = samp_src.crs\n",
    "                    # apply a no-data mask\n",
    "                    mask = samp_data >= 0\n",
    "                    msked_target = np.where(mask, samp_data, -1)\n",
    "                    target_shape = msked_target.shape\n",
    "        \n",
    "                    # flatted data\n",
    "                    samp_flat = msked_target.flatten()\n",
    "                    \n",
    "    \n",
    "                # try to get the fsca variables \n",
    "                sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                for fSCA in os.listdir(fSCAWorkspace):\n",
    "                    if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                        featureName.append(f\"{fSCA[:-4]}\")\n",
    "                        fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                        fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (fsca_norm,)\n",
    "                        # print(fsca_norm.shape)\n",
    "                        if fsca_norm.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "        \n",
    "                # get a DOY array into a feature \n",
    "                date_string = sample.split(\"_\")[1]\n",
    "                doy_str = date_string[-3:]\n",
    "                doy = float(doy_str)\n",
    "                DOY_array = np.full_like(msked_target, doy)\n",
    "                doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                featureTuple += (doy_norm,)\n",
    "                featureName.append(doy)\n",
    "        \n",
    "                # get the vegetation array\n",
    "                for tree in os.listdir(vegetation_path):\n",
    "                    if tree.endswith(extension_filter):\n",
    "                        if tree.startswith(f\"{year}\"):\n",
    "                            featureName.append(f\"{tree[:-4]}\")\n",
    "                            tree_norm = read_aligned_raster(\n",
    "                            src_path=tree_workspace + tree,\n",
    "                            extent=samp_extent,\n",
    "                            target_shape=target_shape\n",
    "                            )\n",
    "                            tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                            featureTuple += (tree_norm,)\n",
    "                            if tree_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                \n",
    "        \n",
    "                # # get all the features in the fodler \n",
    "                for phv in os.listdir(phv_path):\n",
    "                    if phv.endswith(extension_filter):\n",
    "                        featureName.append(f\"{phv[:-4]}\")\n",
    "                        phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                        featureTuple += (phv_data,)\n",
    "                        if phv_data.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "                feature_stack = np.dstack(featureTuple)\n",
    "                if feature_stack.shape[2] != num_of_channels:\n",
    "                    print(f\"⚠️ {sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                    print(featureName)\n",
    "                    print(\" \")\n",
    "                else:\n",
    "                    featureArray.append(feature_stack)\n",
    "                    targetArray.append(samp_flat)\n",
    "                    extent_list.append(samp_extent)\n",
    "                    crs_list.append(samp_crs)\n",
    "        print(\"You go girl!\")\n",
    "        print(\"all data split into target and feature array\")\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973c25e-1351-4229-99b9-b37c404855d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_g2, y_train_g2, train_extents, train_crs = target_feature_stacks_testGroups(year=2022, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group1/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"{year}/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g2, y_test_g2, test_extents, test_crs = target_feature_stacks_testGroups(year = 2022,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group1/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"{year}/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"./best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_mse_loss(no_data_value=-1)\n",
    "})\n",
    "\n",
    "# load train data generator\n",
    "train_generator = DataGenerator(X_train_g2, y_train_g2, batch_size=32)\n",
    "test_generator = DataGenerator(X_test_g2, y_test_g2, batch_size=32)\n",
    "\n",
    "# model predictions\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "for i, pred in enumerate(y_pred):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g1[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"./outTestG1/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=test_extents[i],\n",
    "        crs=test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac44b85-6468-41d3-a8ab-f64129c3fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_g3, y_train_g3, train_extents, train_crs = target_feature_stacks_testGroups(year=2024, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group3/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"{year}/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g3, y_test_g3, test_extents, test_crs = target_feature_stacks_testGroups(year = 2024,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group3/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"{year}/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"./best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_mse_loss(no_data_value=-1)\n",
    "})\n",
    "\n",
    "# load train data generator\n",
    "train_generator = DataGenerator(X_train_g3, y_train_g3, batch_size=32)\n",
    "test_generator = DataGenerator(X_test_g3, y_test_g3, batch_size=32)\n",
    "\n",
    "# model predictions\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "for i, pred in enumerate(y_pred):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g1[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"./outTestG2/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=test_extents[i],\n",
    "        crs=test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef485842-458d-4759-bc12-19eb6398356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_g3, y_train_g3, train_extents, train_crs = target_feature_stacks_testGroups(year=2024, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group3/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"{year}/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g3, y_test_g3, test_extents, test_crs = target_feature_stacks_testGroups(year = 2024,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group3/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"{year}/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"./best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_mse_loss(no_data_value=-1)\n",
    "})\n",
    "\n",
    "# load train data generator\n",
    "train_generator = DataGenerator(X_train_g3, y_train_g3, batch_size=32)\n",
    "test_generator = DataGenerator(X_test_g3, y_test_g3, batch_size=32)\n",
    "\n",
    "# model predictions\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "for i, pred in enumerate(y_pred):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g1[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"./outTestG3/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=test_extents[i],\n",
    "        crs=test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9e354-43d8-49a5-8efe-691ae5f798b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_seconds = end_time - start_time\n",
    "total_minutes = total_seconds / 60\n",
    "print(f\"Training time: {total_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac6d8a-eb1d-4b7a-bb2b-cbf7e27fa823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
