{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac281b-3e91-4fb8-a7c8-d63f4ba8d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseduo code\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "gitBackup = \"N\"\n",
    "years = list(range(2022, 2023))\n",
    "Domain = \"Sierras\"\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1cbad6-2c05-4b05-8d55-e3cd34604924",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gitBackup == \"Y\":\n",
    "    notebook_filename = r\"C:/Users/etyrr/OneDrive/Documents/CU_Grad/ASOML/Code/BigGirlCode_GitCalls.ipynb\"\n",
    "    repo_url = \"https://github.com/EmmaTyrrell/DeepLearningSWE.git\"\n",
    "    \n",
    "    # Initialize repo\n",
    "    subprocess.run([\"git\", \"init\"])\n",
    "    subprocess.run([\"git\", \"remote\", \"add\", \"origin\", repo_url])\n",
    "    \n",
    "    # Add and commit\n",
    "    subprocess.run([\"git\", \"add\", notebook_filename])\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", \"Add notebook\"])\n",
    "    \n",
    "    # Push\n",
    "    subprocess.run([\"git\", \"push\", \"-u\", \"origin\", \"main\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940b914-a48b-4afa-8146-c9135f970ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for min-max scaling\n",
    "def min_max_scale(data, min_val=None, max_val=None, feature_range=(0, 1)):\n",
    "    \"\"\"Min-Max normalize a NumPy array to a target range.\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    mask = np.isnan(data)\n",
    "\n",
    "    d_min = np.nanmin(data) if min_val is None else min_val\n",
    "    d_max = np.nanmax(data) if max_val is None else max_val\n",
    "\n",
    "    # if d_max == d_min:\n",
    "    #     raise ValueError(\"Min and max are equal — can't scale.\")\n",
    "    if d_max == d_min:\n",
    "        return np.full_like(data, feature_range[0], dtype=np.float32)\n",
    "\n",
    "    a, b = feature_range\n",
    "    scaled = (data - d_min) / (d_max - d_min)  # to [0, 1]\n",
    "    scaled = scaled * (b - a) + a              # to [a, b]\n",
    "\n",
    "    scaled[mask] = np.nan  # preserve NaNs\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcff9b-f05b-4de4-a798-4abd5c94d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import from_bounds\n",
    "\n",
    "def read_aligned_raster(src_path, extent, target_shape, nodata_val=-1):\n",
    "    height, width = target_shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        try:\n",
    "            data = src.read(\n",
    "                1,\n",
    "                out_shape=target_shape,\n",
    "                resampling=rasterio.enums.Resampling.nearest,\n",
    "                window=src.window(*extent)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {src_path}: {e}\")\n",
    "            return np.full(target_shape, nodata_val, dtype=np.float32)\n",
    "\n",
    "        # Handle nodata in source\n",
    "        src_nodata = src.nodata\n",
    "        if src_nodata is not None:\n",
    "            data = np.where(data == src_nodata, np.nan, data)\n",
    "\n",
    "        # Replace NaNs or invalid with -1\n",
    "        data = np.where(np.isnan(data), nodata_val, data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa5377-0011-4db5-9dd1-b6f87e72fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_array_as_raster(output_path, array, extent, crs, nodata_val=-1):\n",
    "    height, width = array.shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=array.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "        nodata=nodata_val\n",
    "    ) as dst:\n",
    "        dst.write(array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57c3d3-fd37-4067-a32b-4d53fb69db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the features and arrarys \n",
    "\n",
    "## create empty arrays\n",
    "featureArray = []\n",
    "targetArray = []\n",
    "\n",
    "# loop through the years and feature data\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}\")\n",
    "    targetSplits = WorkspaceBase + f\"{year}/SWE_processed_splits/\"\n",
    "    fSCAWorkspace = WorkspaceBase + f\"{year}/fSCA/\"\n",
    "    for sample in os.listdir(targetSplits):\n",
    "        featureTuple = ()\n",
    "        featureName = []\n",
    "        # loop through each sample and get the corresponding features\n",
    "        if sample.endswith(\"nonull_fnl.tif\"):\n",
    "            # read in data\n",
    "            with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                samp_data = samp_src.read(1)\n",
    "                meta = samp_src.meta.copy()\n",
    "                samp_extent = samp_src.bounds\n",
    "                samp_transform = samp_src.transform\n",
    "                samp_crs = samp_src.crs\n",
    "    \n",
    "                # apply a mask to all no data values. Reminder that nodata values is -9999\n",
    "                mask = samp_data >= 0\n",
    "                msked_target = np.where(mask, samp_data, -1)\n",
    "                target_shape = msked_target.shape\n",
    "    \n",
    "                # flatted data\n",
    "                samp_flat = msked_target.flatten()\n",
    "                \n",
    "\n",
    "            # try to get the fsca variables \n",
    "            sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "            for fSCA in os.listdir(fSCAWorkspace):\n",
    "                if fSCA.endswith(\".tif\") and fSCA.startswith(sample_root):\n",
    "                    featureName.append(f\"{fSCA[:-4]}\")\n",
    "                    fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                    fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                    featureTuple += (fsca_norm,)\n",
    "                    # print(fsca_norm.shape)\n",
    "                    if fsca_norm.shape != (256, 256):\n",
    "                        print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                        output_debug_path = f\"./debug_output/{sample_root}_BAD_FSCA.tif\"\n",
    "                        save_array_as_raster(\n",
    "                            output_path=output_debug_path,\n",
    "                            array=fsca_norm,\n",
    "                            extent=samp_extent,\n",
    "                            crs=samp_crs,\n",
    "                            nodata_val=-1\n",
    "                        )\n",
    "    \n",
    "            # get a DOY array into a feature \n",
    "            date_string = sample.split(\"_\")[1]\n",
    "            doy_str = date_string[-3:]\n",
    "            doy = float(doy_str)\n",
    "            DOY_array = np.full_like(msked_target, doy)\n",
    "            doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "            featureTuple += (doy_norm,)\n",
    "            featureName.append(doy)\n",
    "    \n",
    "            # get the vegetation array\n",
    "            for tree in os.listdir(tree_workspace):\n",
    "                if tree.endswith(\".tif\"):\n",
    "                    if tree.startswith(f\"{year}\"):\n",
    "                        featureName.append(f\"{tree[:-4]}\")\n",
    "                        tree_norm = read_aligned_raster(\n",
    "                        src_path=tree_workspace + tree,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (tree_norm,)\n",
    "                        if tree_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                            output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            \n",
    "    \n",
    "            # # get all the features in the fodler \n",
    "            for phv in os.listdir(phv_features):\n",
    "                if phv.endswith(\".tif\"):\n",
    "                    featureName.append(f\"{phv[:-4]}\")\n",
    "                    phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                    featureTuple += (phv_data,)\n",
    "                    if phv_data.shape != (256, 256):\n",
    "                         print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                        \n",
    "            feature_stack = np.dstack(featureTuple)\n",
    "            if feature_stack.shape[2] != 14:\n",
    "                print(f\"⚠️ {sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                print(featureName)\n",
    "                print(\" \")\n",
    "            else:\n",
    "                featureArray.append(feature_stack)\n",
    "                targetArray.append(samp_flat)\n",
    "    print(\"You go girl!\")\n",
    "X = np.array(featureArray)\n",
    "y = np.array(targetArray)\n",
    "print(\"all data split into target and feature array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f139f-1221-481a-94c2-d97dbc2ad56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of input data\")\n",
    "print(f\"feature shape: {X.shape}\")\n",
    "print(f\"target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580cd460-4fee-447c-950d-718f514018c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split between training and test data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "print(\" \")\n",
    "print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a2dc07-a97b-4965-9e2f-7f30270beb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(256, 256, 14), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "# add stacked convolutions\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# dense layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "# output layer\n",
    "model.add(Dense(65536, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3571fc1-704a-47d9-988a-169dfe11fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def masked_mse_loss(no_data_value=-1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Create mask where y_true != no_data_value\n",
    "        mask = tf.not_equal(y_true, no_data_value)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "        # Apply mask\n",
    "        squared_diff = tf.square(y_true - y_pred) * mask\n",
    "\n",
    "        # Avoid division by zero\n",
    "        loss_value = tf.reduce_sum(squared_diff) / tf.reduce_sum(mask)\n",
    "        return loss_value\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa7116-6318-42c9-a600-f0bce472d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Fetch batch data\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "\n",
    "        # Optionally, do preprocessing or masking here\n",
    "        # (e.g., ensure y_batch is ready for custom loss with -1 masking)\n",
    "        y_batch[y_batch == -1] = 0\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7d4ab-d763-47c5-8ffd-6f8f37953ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build in early stops\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"./best_model_{timestamp}.keras\", monitor=\"val_loss\",\n",
    "    verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss=masked_mse_loss(no_data_value=-1))\n",
    "\n",
    "# get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb25ac2-5780-4744-b7c4-b1e74f37ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish the model\n",
    "batch_size = 32\n",
    "train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "# model git\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33daad-ddc0-4900-a149-78481ee6aade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
