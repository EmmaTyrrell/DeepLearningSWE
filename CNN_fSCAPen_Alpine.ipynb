{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ac281b-3e91-4fb8-a7c8-d63f4ba8d0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules established\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# create folder for model outputs\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelOuptuts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m inter_model_outWorkspace \u001b[38;5;241m=\u001b[39m modelOuptuts \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(timestamp)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aso-dl\\lib\\os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aso-dl\\lib\\os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: makedirs at line 215 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aso-dl\\lib\\os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aso-dl\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:/'"
     ]
    }
   ],
   "source": [
    "# pseduo code\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "years = list(range(2022, 2025))\n",
    "Domain = \"Sierras\"\n",
    "WorkspaceBase = \"/pl/active/hydroServer/ASO_DeepLearningData/CNN_v.02/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "final_activation = 'relu'\n",
    "\n",
    "## seting folder\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# create folder for model outputs\n",
    "os.makedirs(modelOuptuts + f\"{str(timestamp)}/\", exist_ok=True)\n",
    "inter_model_outWorkspace = modelOuptuts + f\"{str(timestamp)}/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940b914-a48b-4afa-8146-c9135f970ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for min-max scaling\n",
    "def min_max_scale(data, min_val=None, max_val=None, feature_range=(0, 1)):\n",
    "    \"\"\"Min-Max normalize a NumPy array to a target range.\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    mask = np.isnan(data)\n",
    "\n",
    "    d_min = np.nanmin(data) if min_val is None else min_val\n",
    "    d_max = np.nanmax(data) if max_val is None else max_val\n",
    "\n",
    "    # if d_max == d_min:\n",
    "    #     raise ValueError(\"Min and max are equal â€” can't scale.\")\n",
    "    if d_max == d_min:\n",
    "        return np.full_like(data, feature_range[0], dtype=np.float32)\n",
    "\n",
    "    a, b = feature_range\n",
    "    scaled = (data - d_min) / (d_max - d_min)  # to [0, 1]\n",
    "    scaled = scaled * (b - a) + a              # to [a, b]\n",
    "\n",
    "    scaled[mask] = np.nan  # preserve NaNs\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcff9b-f05b-4de4-a798-4abd5c94d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_aligned_raster(src_path, extent, target_shape, nodata_val=-1):\n",
    "    height, width = target_shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        try:\n",
    "            data = src.read(\n",
    "                1,\n",
    "                out_shape=target_shape,\n",
    "                resampling=rasterio.enums.Resampling.nearest,\n",
    "                window=src.window(*extent)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {src_path}: {e}\")\n",
    "            return np.full(target_shape, nodata_val, dtype=np.float32)\n",
    "\n",
    "        # Handle nodata in source\n",
    "        src_nodata = src.nodata\n",
    "        if src_nodata is not None:\n",
    "            data = np.where(data == src_nodata, np.nan, data)\n",
    "\n",
    "        # Replace NaNs or invalid with -1\n",
    "        data = np.where(np.isnan(data), nodata_val, data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa5377-0011-4db5-9dd1-b6f87e72fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_array_as_raster(output_path, array, extent, crs, nodata_val=-1):\n",
    "    height, width = array.shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=array.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "        nodata=nodata_val\n",
    "    ) as dst:\n",
    "        dst.write(array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d48e8e-f2e4-4407-9d6b-0d54948a7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the features and arrarys \n",
    "\n",
    "## create empty arrays\n",
    "featureArray = []\n",
    "targetArray = []\n",
    "\n",
    "# loop through the years and feature data\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}\")\n",
    "    targetSplits = WorkspaceBase + f\"{year}/SWE_processed_splits/\"\n",
    "    fSCAWorkspace = WorkspaceBase + f\"{year}/fSCA/\"\n",
    "    for sample in os.listdir(targetSplits):\n",
    "        featureTuple = ()\n",
    "        featureName = []\n",
    "        # loop through each sample and get the corresponding features\n",
    "        if sample.endswith(\"nonull_fnl.tif\"):\n",
    "            # read in data\n",
    "            with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                samp_data = samp_src.read(1)\n",
    "                meta = samp_src.meta.copy()\n",
    "                samp_extent = samp_src.bounds\n",
    "                samp_transform = samp_src.transform\n",
    "                samp_crs = samp_src.crs\n",
    "    \n",
    "                # apply a mask to all no data values. Reminder that nodata values is -9999\n",
    "                mask = samp_data >= 0\n",
    "                msked_target = np.where(mask, samp_data, -1)\n",
    "                target_shape = msked_target.shape\n",
    "    \n",
    "                # flatted data\n",
    "                samp_flat = msked_target.flatten()\n",
    "                \n",
    "\n",
    "            # try to get the fsca variables \n",
    "            sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "            for fSCA in os.listdir(fSCAWorkspace):\n",
    "                if fSCA.endswith(\".tif\") and fSCA.startswith(sample_root):\n",
    "                    featureName.append(f\"{fSCA[:-4]}\")\n",
    "                    fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                    fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                    featureTuple += (fsca_norm,)\n",
    "                    # print(fsca_norm.shape)\n",
    "                    fsca_flat = fsca_norm.flatten()\n",
    "                    fsca_binary_mask = (fsca_flat > 0).astype(np.float32)\n",
    "                    if fsca_norm.shape != (256, 256):\n",
    "                        print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                        output_debug_path = f\"./debug_output/{sample_root}_BAD_FSCA.tif\"\n",
    "                        save_array_as_raster(\n",
    "                            output_path=output_debug_path,\n",
    "                            array=fsca_norm,\n",
    "                            extent=samp_extent,\n",
    "                            crs=samp_crs,\n",
    "                            nodata_val=-1\n",
    "                        )\n",
    "    \n",
    "            # get a DOY array into a feature \n",
    "            date_string = sample.split(\"_\")[1]\n",
    "            doy_str = date_string[-3:]\n",
    "            doy = float(doy_str)\n",
    "            DOY_array = np.full_like(msked_target, doy)\n",
    "            doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "            featureTuple += (doy_norm,)\n",
    "            featureName.append(doy)\n",
    "    \n",
    "            # get the vegetation array\n",
    "            for tree in os.listdir(tree_workspace):\n",
    "                if tree.endswith(\".tif\"):\n",
    "                    if tree.startswith(f\"{year}\"):\n",
    "                        featureName.append(f\"{tree[:-4]}\")\n",
    "                        tree_norm = read_aligned_raster(\n",
    "                        src_path=tree_workspace + tree,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (tree_norm,)\n",
    "                        if tree_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                            output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            \n",
    "            # # get all the features in the fodler \n",
    "            for phv in os.listdir(phv_features):\n",
    "                if phv.endswith(\".tif\"):\n",
    "                    featureName.append(f\"{phv[:-4]}\")\n",
    "                    phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                    featureTuple += (phv_data,)\n",
    "                    if phv_data.shape != (256, 256):\n",
    "                         print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                        \n",
    "            feature_stack = np.dstack(featureTuple)\n",
    "            if feature_stack.shape[2] != 14:\n",
    "                print(f\"{sample} has shape {feature_stack.shape} â€” missing or extra feature?\")\n",
    "                print(featureName)\n",
    "                print(\" \")\n",
    "            else:\n",
    "                featureArray.append(feature_stack)\n",
    "                target_combined = np.concatenate([samp_flat, fsca_binary_mask])\n",
    "                targetArray.append(target_combined)\n",
    "                # targetArray.append(samp_flat)\n",
    "    print(\"You go girl!\")\n",
    "X = np.array(featureArray)\n",
    "y = np.array(targetArray)\n",
    "print(\"all data split into target and feature array\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83160ba4-d47a-4698-9b94-ed9edde8993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "def masked_loss_fn(y_true, y_pred, loss_fn, mask_value=-1):\n",
    "    # Use tf.shape instead of .shape attribute for graph compatibility\n",
    "    y_true_shape = tf.shape(y_true)[1]\n",
    "    y_pred_shape = tf.shape(y_pred)[1]\n",
    "    \n",
    "    # Safely extract SWE part using dynamic slicing\n",
    "    y_true_swe = tf.cond(\n",
    "        tf.greater(y_true_shape, y_pred_shape),\n",
    "        lambda: y_true[:, :y_pred_shape],\n",
    "        lambda: y_true\n",
    "    )\n",
    "    \n",
    "    # Create mask for valid values\n",
    "    mask = tf.not_equal(y_true_swe, mask_value)\n",
    "    \n",
    "    # Use boolean_mask with safety checks\n",
    "    y_true_masked = tf.boolean_mask(y_true_swe, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    \n",
    "    # Apply the loss function\n",
    "    return loss_fn(y_true_masked, y_pred_masked)\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mse\")\n",
    "def masked_mse(y_true, y_pred):\n",
    "    # Create loss function instance outside the function call for stability\n",
    "    mse_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    return masked_loss_fn(y_true, y_pred, mse_fn, mask_value=-1)\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mae\")\n",
    "def masked_mae(y_true, y_pred):\n",
    "    mae_fn = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    return masked_loss_fn(y_true, y_pred, mae_fn, mask_value=-1)\n",
    "\n",
    "@register_keras_serializable(name=\"masked_rmse\")\n",
    "def masked_rmse(y_true, y_pred):\n",
    "    mse_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    mse = masked_loss_fn(y_true, y_pred, mse_fn, mask_value=-1)\n",
    "    return tf.sqrt(mse)\n",
    "    \n",
    "@register_keras_serializable(name=\"constrained_masked_mse\")\n",
    "def constrained_masked_mse(mask_value=-1, penalty_weight=10.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Get dynamic shapes\n",
    "        y_true_shape = tf.shape(y_true)\n",
    "        batch_size = y_true_shape[0]\n",
    "        \n",
    "        # Extract SWE values and fSCA mask from y_true\n",
    "        half_size = 65536  # 256*256\n",
    "        swe_true = y_true[:, :half_size]  # First half is SWE values\n",
    "        fsca_mask = y_true[:, half_size:]  # Second half is fSCA mask\n",
    "        \n",
    "        # Create mask for valid SWE values (not equal to mask_value)\n",
    "        valid_mask = tf.not_equal(swe_true, mask_value)\n",
    "        \n",
    "        # Apply masking to calculate base MSE loss\n",
    "        swe_true_masked = tf.boolean_mask(swe_true, valid_mask)\n",
    "        y_pred_masked = tf.boolean_mask(y_pred, valid_mask)\n",
    "        \n",
    "        # Base MSE loss\n",
    "        mse = tf.reduce_mean(tf.square(swe_true_masked - y_pred_masked))\n",
    "        \n",
    "        # Reshape predictions and fSCA mask to 2D images for constraint calculation\n",
    "        y_pred_img = tf.reshape(y_pred, [batch_size, 256, 256])\n",
    "        fsca_img = tf.reshape(fsca_mask, [batch_size, 256, 256])\n",
    "        \n",
    "        # Calculate constraint violation: SWE <= 0 where fSCA > 0\n",
    "        violation_mask = tf.logical_and(fsca_img > 0, y_pred_img <= 0)\n",
    "        \n",
    "        # Count violations relative to number of fSCA pixels\n",
    "        violation_count = tf.reduce_sum(tf.cast(violation_mask, tf.float32), axis=[1, 2])\n",
    "        total_fsca_pixels = tf.reduce_sum(tf.cast(fsca_img > 0, tf.float32), axis=[1, 2])\n",
    "        \n",
    "        # Avoid division by zero with a small epsilon\n",
    "        epsilon = 1e-7\n",
    "        safe_total = total_fsca_pixels + epsilon\n",
    "        \n",
    "        # Calculate violation ratio\n",
    "        violation_ratio = violation_count / safe_total\n",
    "        \n",
    "        # Apply penalty\n",
    "        penalty = tf.reduce_mean(violation_ratio) * penalty_weight\n",
    "        \n",
    "        # The final loss is MSE + penalty\n",
    "        return mse + penalty\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@register_keras_serializable(name=\"constraint_violation_ratio\")\n",
    "def constraint_violation_ratio(y_true, y_pred):\n",
    "    # Get dynamic shapes\n",
    "    batch_size = tf.shape(y_true)[0]\n",
    "    half_size = 65536  # 256*256\n",
    "    \n",
    "    # Extract SWE and fSCA from y_true\n",
    "    fsca_mask = y_true[:, half_size:]\n",
    "    \n",
    "    # Reshape to 2D images\n",
    "    y_pred_img = tf.reshape(y_pred, [batch_size, 256, 256])\n",
    "    fsca_img = tf.reshape(fsca_mask, [batch_size, 256, 256])\n",
    "    \n",
    "    # Calculate violation ratio\n",
    "    violation_mask = tf.logical_and(fsca_img > 0, y_pred_img <= 0)\n",
    "    violation_count = tf.reduce_sum(tf.cast(violation_mask, tf.float32), axis=[1, 2])\n",
    "    total_fsca_pixels = tf.reduce_sum(tf.cast(fsca_img > 0, tf.float32), axis=[1, 2])\n",
    "    \n",
    "    # Avoid division by zero with small epsilon\n",
    "    epsilon = 1e-7\n",
    "    safe_total = total_fsca_pixels + epsilon\n",
    "    \n",
    "    return tf.reduce_mean(violation_count / safe_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bccd62-c333-4ead-b9b2-ec2ae97c9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNOWDataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True, augment=False, mask_value=-1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.mask_value = mask_value\n",
    "        self.indices = np.arange(len(X))\n",
    "        \n",
    "        # Verify data shapes\n",
    "        print(f\"X shape: {X.shape}\")\n",
    "        print(f\"y shape: {y.shape}\")\n",
    "        \n",
    "        # Make sure y has the right shape (each sample should be [SWE, fSCA])\n",
    "        if len(y.shape) == 2 and y.shape[1] == 65536 * 2:\n",
    "            print(\"Target data has correct shape for SWE + fSCA mask\")\n",
    "        else:\n",
    "            print(f\"WARNING: Target data shape {y.shape} might not be compatible\")\n",
    "            \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.X))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        # Fetch batch data\n",
    "        X_batch = self.X[batch_indices].copy()\n",
    "        y_batch = self.y[batch_indices].copy()\n",
    "        \n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            X_batch, y_batch = self._augment_batch(X_batch, y_batch)\n",
    "        \n",
    "        # Debug: Print shapes before returning\n",
    "        # print(f\"Batch X shape: {X_batch.shape}, y shape: {y_batch.shape}\")\n",
    "            \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def _augment_batch(self, X_batch, y_batch):\n",
    "        \"\"\"Apply data augmentation to the batch\"\"\"\n",
    "        for i in range(len(X_batch)):\n",
    "            # 50% chance to apply an augmentation\n",
    "            if np.random.random() > 0.5:\n",
    "                # Choose a random transformation\n",
    "                choice = np.random.randint(0, 3)\n",
    "                \n",
    "                if choice == 0:  # Horizontal flip\n",
    "                    X_batch[i] = X_batch[i, :, ::-1, :]\n",
    "                    # Split y and handle separately\n",
    "                    swe = y_batch[i, :65536].reshape(256, 256)\n",
    "                    fsca = y_batch[i, 65536:].reshape(256, 256)\n",
    "                    swe = swe[:, ::-1]\n",
    "                    fsca = fsca[:, ::-1]\n",
    "                    y_batch[i, :65536] = swe.flatten()\n",
    "                    y_batch[i, 65536:] = fsca.flatten()\n",
    "                    \n",
    "                elif choice == 1:  # Vertical flip\n",
    "                    X_batch[i] = X_batch[i, ::-1, :, :]\n",
    "                    swe = y_batch[i, :65536].reshape(256, 256)\n",
    "                    fsca = y_batch[i, 65536:].reshape(256, 256)\n",
    "                    swe = swe[::-1, :]\n",
    "                    fsca = fsca[::-1, :]\n",
    "                    y_batch[i, :65536] = swe.flatten()\n",
    "                    y_batch[i, 65536:] = fsca.flatten()\n",
    "                    \n",
    "                elif choice == 2:  # Random 90-degree rotation\n",
    "                    k = np.random.randint(1, 4)  # 1, 2, or 3 for 90, 180, 270 degrees\n",
    "                    X_batch[i] = np.rot90(X_batch[i], k=k, axes=(0, 1))\n",
    "                    swe = y_batch[i, :65536].reshape(256, 256)\n",
    "                    fsca = y_batch[i, 65536:].reshape(256, 256)\n",
    "                    swe = np.rot90(swe, k=k)\n",
    "                    fsca = np.rot90(fsca, k=k)\n",
    "                    y_batch[i, :65536] = swe.flatten()\n",
    "                    y_batch[i, 65536:] = fsca.flatten()\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "# to gradually increase the weights\n",
    "class ConstraintWeightScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_weight=1.0, max_weight=100.0, increase_per_epoch=5.0):\n",
    "        super().__init__()\n",
    "        self.initial_weight = initial_weight\n",
    "        self.max_weight = max_weight\n",
    "        self.increase_per_epoch = increase_per_epoch\n",
    "        self.current_weight = initial_weight\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Calculate new weight\n",
    "        self.current_weight = min(\n",
    "            self.initial_weight + epoch * self.increase_per_epoch,\n",
    "            self.max_weight\n",
    "        )\n",
    "        \n",
    "        # Get the loss function - safely handle custom objects with closures\n",
    "        loss_fn = self.model.loss\n",
    "        \n",
    "        # Since we can't directly update the weight variable,\n",
    "        # print a message recommending using a LearningRateScheduler instead\n",
    "        print(f\"\\nEpoch {epoch+1}: Constraint penalty weight should be {self.current_weight:.2f}\")\n",
    "        print(\"Note: To dynamically change penalty weight, consider implementing a custom training loop\")\n",
    "\n",
    "class GradientMonitoringModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(\n",
    "                y, y_pred, regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "\n",
    "        # Use tf ops, not .numpy() (because we're in graph mode)\n",
    "        gradient_norms = [tf.norm(g) if g is not None else tf.constant(0.0) for g in gradients]\n",
    "\n",
    "        max_norm = tf.reduce_max(gradient_norms)\n",
    "        min_norm = tf.reduce_min(gradient_norms)\n",
    "        mean_norm = tf.reduce_mean(gradient_norms)\n",
    "\n",
    "        # Log using tf.print so it works in graph mode\n",
    "        tf.print(\n",
    "            \"\\nGradient norms - Max:\", max_norm,\n",
    "            \"Min:\", min_norm,\n",
    "            \"Mean:\", mean_norm\n",
    "        )\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38bc87-90a3-4e4a-81d3-4438fcfe8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of input data\")\n",
    "print(f\"feature shape: {X.shape}\")\n",
    "print(f\"target shape: {y.shape}\")\n",
    "\n",
    "# split between training and test data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "print(\" \")\n",
    "print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "print(\"***\")\n",
    "\n",
    "# Build Network\n",
    "inputs = Input(shape=(256, 256, 14))\n",
    "\n",
    "x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='valid')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='valid')(x)\n",
    "\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='valid')(x)\n",
    "\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='valid')(x)\n",
    "\n",
    "x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='valid')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2), padding='valid')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(65536, activation=final_activation)(x)\n",
    "\n",
    "# Wrap in your custom model subclass\n",
    "model = GradientMonitoringModel(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e9f8f5-e8c9-4439-82f8-7efa029bc9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gradient_monitoring_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 14)]    0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 254, 254, 64)      8128      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 254, 254, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 128)     73856     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 125, 125, 128)    512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 60, 60, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              25691136  \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 65536)             67174400  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,248,576\n",
      "Trainable params: 93,245,632\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# uploading and saving model stats\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Compile the model with the loss function\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=constrained_masked_mse(mask_value=-1, penalty_weight=10.0),\n",
    "    metrics=[masked_rmse, masked_mae, masked_mse, constraint_violation_ratio]\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\",\n",
    "    monitor=\"val_loss\",  \n",
    "    verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode='min', verbose=1, patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=f\"{inter_model_outWorkspace}/logs_{timestamp}\",\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    update_freq='epoch'\n",
    ")\n",
    "\n",
    "# get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf6a83-67fd-4414-a990-1e16c87c9420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (940, 256, 256, 14)\n",
      "y shape: (940, 131072)\n",
      "Target data has correct shape for SWE + fSCA mask\n",
      "X shape: (166, 256, 256, 14)\n",
      "y shape: (166, 131072)\n",
      "Target data has correct shape for SWE + fSCA mask\n",
      "Epoch 1/100\n",
      "\n",
      "Gradient norms - Max: 0.0753051266 Min: 0.000168458297 Mean: 0.0140555613\n",
      " 1/30 [>.............................] - ETA: 5:04 - loss: 4.9283 - masked_rmse: 0.4988 - masked_mae: 0.3700 - masked_mse: 0.2488 - constraint_violation_ratio: 0.4680\n",
      "Gradient norms - Max: 0.178570941 Min: 0.00062865566 Mean: 0.038293913\n",
      " 2/30 [=>............................] - ETA: 1:31 - loss: 4.9011 - masked_rmse: 0.4650 - masked_mae: 0.3373 - masked_mse: 0.2173 - constraint_violation_ratio: 0.4684\n",
      "Gradient norms - Max: 0.101657622 Min: 0.000220726652 Mean: 0.0215456113\n",
      " 3/30 [==>...........................] - ETA: 1:22 - loss: 4.6417 - masked_rmse: 0.4263 - masked_mae: 0.3043 - masked_mse: 0.1855 - constraint_violation_ratio: 0.4456\n",
      "Gradient norms - Max: 0.255201876 Min: 0.000431962137 Mean: 0.0508387946\n",
      " 4/30 [===>..........................] - ETA: 1:17 - loss: 4.6831 - masked_rmse: 0.4004 - masked_mae: 0.2931 - masked_mse: 0.1651 - constraint_violation_ratio: 0.4518\n",
      "Gradient norms - Max: 0.36218968 Min: 0.000738828734 Mean: 0.0731838569\n",
      " 5/30 [====>.........................] - ETA: 1:13 - loss: 4.3757 - masked_rmse: 0.4101 - masked_mae: 0.3000 - masked_mse: 0.1724 - constraint_violation_ratio: 0.4203\n",
      "Gradient norms - Max: 0.0846320838 Min: 0.000178021743 Mean: 0.0185481831\n",
      " 6/30 [=====>........................] - ETA: 1:10 - loss: 4.1956 - masked_rmse: 0.3968 - masked_mae: 0.2875 - masked_mse: 0.1619 - constraint_violation_ratio: 0.4034\n",
      "Gradient norms - Max: 0.0622921698 Min: 0.000142325924 Mean: 0.0132699525\n",
      " 7/30 [======>.......................] - ETA: 1:07 - loss: 4.0196 - masked_rmse: 0.3835 - masked_mae: 0.2758 - masked_mse: 0.1519 - constraint_violation_ratio: 0.3868\n",
      "Gradient norms - Max: 0.0692804828 Min: 0.000114752082 Mean: 0.0128592402\n",
      " 8/30 [=======>......................] - ETA: 1:04 - loss: 3.9178 - masked_rmse: 0.3739 - masked_mae: 0.2669 - masked_mse: 0.1447 - constraint_violation_ratio: 0.3773\n",
      "Gradient norms - Max: 0.105941154 Min: 0.000125285515 Mean: 0.017484976\n",
      " 9/30 [========>.....................] - ETA: 1:01 - loss: 3.8243 - masked_rmse: 0.3698 - masked_mae: 0.2628 - masked_mse: 0.1413 - constraint_violation_ratio: 0.3683\n",
      "Gradient norms - Max: 0.0481170639 Min: 0.000103483828 Mean: 0.00948406\n",
      "10/30 [=========>....................] - ETA: 58s - loss: 3.7321 - masked_rmse: 0.3636 - masked_mae: 0.2577 - masked_mse: 0.1366 - constraint_violation_ratio: 0.3596 "
     ]
    }
   ],
   "source": [
    "# establish the model\n",
    "batch_size = 4\n",
    "train_gen = SNOWDataGenerator(X_train, y_train, batch_size=32, shuffle=True, augment=True)\n",
    "valid_gen = SNOWDataGenerator(X_valid, y_valid, batch_size=32, shuffle=False, augment=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=valid_gen,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33daad-ddc0-4900-a149-78481ee6aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Main loss plot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# RMSE plot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['masked_rmse'])\n",
    "plt.plot(history.history['val_masked_rmse'])\n",
    "plt.title('Masked RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# MAE plot\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history.history['masked_mae'])\n",
    "plt.plot(history.history['val_masked_mae'])\n",
    "plt.title('Masked MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Constraint violation plot\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history.history['constraint_violation_ratio'])\n",
    "plt.plot(history.history['val_constraint_violation_ratio'])\n",
    "plt.title('Constraint Violation Ratio')\n",
    "plt.ylabel('Ratio')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{inter_model_outWorkspace}/training_history_{timestamp}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cf1c1-e07d-4343-b27d-736766475e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model.fit() has completed\n",
    "f = open(inter_model_outWorkspace + \"output.txt\", \"a\")\n",
    "sys.stdout = f\n",
    "metrics_to_track = ['val_masked_rmse', 'val_masked_mse', 'val_masked_mae']\n",
    "\n",
    "print(\"\\n Validation Metric Progression:\")\n",
    "for metric in metrics_to_track:\n",
    "    values = history.history.get(metric, [])\n",
    "    if values:\n",
    "        print(f\"{metric}: Start = {values[0]:.4f}, End = {values[-1]:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: Not found in history.\")\n",
    "print(f\"Final activation function: {final_activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e27f96-8b01-4128-be68-baa4fa2460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks(start_year, end_year, target_splits_path, fSCA_path, vegetation_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        years = list(range(start_year, (int(end_year) + 1)))\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        for year in years:\n",
    "            print(f\"Processing {year}\")\n",
    "            targetSplits = target_splits_path\n",
    "            fSCAWorkspace = fSCA_path\n",
    "            for sample in os.listdir(targetSplits):\n",
    "                featureTuple = ()\n",
    "                featureName = []\n",
    "                # loop through each sample and get the corresponding features\n",
    "                if sample.endswith(extension_filter):\n",
    "                    # read in data\n",
    "                    with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                        samp_data = samp_src.read(1)\n",
    "                        meta = samp_src.meta.copy()\n",
    "                        samp_extent = samp_src.bounds\n",
    "                        samp_transform = samp_src.transform\n",
    "                        samp_crs = samp_src.crs\n",
    "                        # apply a no-data mask\n",
    "                        mask = samp_data >= 0\n",
    "                        msked_target = np.where(mask, samp_data, -1)\n",
    "                        target_shape = msked_target.shape\n",
    "            \n",
    "                        # flatted data\n",
    "                        samp_flat = msked_target.flatten()\n",
    "                        \n",
    "        \n",
    "                    # try to get the fsca variables \n",
    "                    sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                    for fSCA in os.listdir(fSCAWorkspace):\n",
    "                        if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                            featureName.append(f\"{fSCA[:-4]}\")\n",
    "                            fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                            fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                            fsca_flat = fsca_norm.flatten()\n",
    "                            fsca_binary_mask = (fsca_flat > 0).astype(np.float32)\n",
    "                            featureTuple += (fsca_norm,)\n",
    "                            # print(fsca_norm.shape)\n",
    "                            if fsca_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "            \n",
    "                    # get a DOY array into a feature \n",
    "                    date_string = sample.split(\"_\")[1]\n",
    "                    doy_str = date_string[-3:]\n",
    "                    doy = float(doy_str)\n",
    "                    DOY_array = np.full_like(msked_target, doy)\n",
    "                    doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                    featureTuple += (doy_norm,)\n",
    "                    featureName.append(doy)\n",
    "            \n",
    "                    # get the vegetation array\n",
    "                    for tree in os.listdir(vegetation_path):\n",
    "                        if tree.endswith(extension_filter):\n",
    "                            if tree.startswith(f\"{year}\"):\n",
    "                                featureName.append(f\"{tree[:-4]}\")\n",
    "                                tree_norm = read_aligned_raster(\n",
    "                                src_path=tree_workspace + tree,\n",
    "                                extent=samp_extent,\n",
    "                                target_shape=target_shape\n",
    "                                )\n",
    "                                tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                                featureTuple += (tree_norm,)\n",
    "                                if tree_norm.shape != desired_shape:\n",
    "                                    print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                    output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                    save_array_as_raster(\n",
    "                                        output_path=output_debug_path,\n",
    "                                        array=fsca_norm,\n",
    "                                        extent=samp_extent,\n",
    "                                        crs=samp_crs,\n",
    "                                        nodata_val=-1\n",
    "                                    )\n",
    "                    \n",
    "            \n",
    "                    # # get all the features in the fodler \n",
    "                    for phv in os.listdir(phv_path):\n",
    "                        if phv.endswith(extension_filter):\n",
    "                            featureName.append(f\"{phv[:-4]}\")\n",
    "                            phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                            featureTuple += (phv_data,)\n",
    "                            if phv_data.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                    feature_stack = np.dstack(featureTuple)\n",
    "                    if feature_stack.shape[2] != num_of_channels:\n",
    "                        print(f\"{sample} has shape {feature_stack.shape} â€” missing or extra feature?\")\n",
    "                        print(featureName)\n",
    "                        print(\" \")\n",
    "                    else:\n",
    "                        featureArray.append(feature_stack)\n",
    "                        target_combined = np.concatenate([samp_flat, fsca_binary_mask])\n",
    "                        targetArray.append(target_combined)\n",
    "                        # targetArray.append(samp_flat)\n",
    "                        extent_list.append(samp_extent)\n",
    "                        crs_list.append(samp_crs)\n",
    "            print(\"You go girl!\")\n",
    "        print(\"all data split into target and feature array\")\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1b35e-4777-4532-a516-3022e3a440e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks_testGroups(year, target_splits_path, fSCA_path, vegetation_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        # print(f\"Processing {group}\")\n",
    "        targetSplits = target_splits_path\n",
    "        fSCAWorkspace = fSCA_path\n",
    "        for sample in os.listdir(targetSplits):\n",
    "            featureTuple = ()\n",
    "            featureName = []\n",
    "            # loop through each sample and get the corresponding features\n",
    "            if sample.endswith(extension_filter):\n",
    "                # read in data\n",
    "                with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                    samp_data = samp_src.read(1)\n",
    "                    meta = samp_src.meta.copy()\n",
    "                    samp_extent = samp_src.bounds\n",
    "                    samp_transform = samp_src.transform\n",
    "                    samp_crs = samp_src.crs\n",
    "                    # apply a no-data mask\n",
    "                    mask = samp_data >= 0\n",
    "                    msked_target = np.where(mask, samp_data, -1)\n",
    "                    target_shape = msked_target.shape\n",
    "        \n",
    "                    # flatted data\n",
    "                    samp_flat = msked_target.flatten()\n",
    "                    \n",
    "    \n",
    "                # try to get the fsca variables \n",
    "                sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                for fSCA in os.listdir(fSCAWorkspace):\n",
    "                    if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                        featureName.append(f\"{fSCA[:-4]}\")\n",
    "                        fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                        fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                        fsca_flat = fsca_norm.flatten()\n",
    "                        fsca_binary_mask = (fsca_flat > 0).astype(np.float32)\n",
    "                        featureTuple += (fsca_norm,)\n",
    "                        # print(fsca_norm.shape)\n",
    "                        if fsca_norm.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "        \n",
    "                # get a DOY array into a feature \n",
    "                date_string = sample.split(\"_\")[1]\n",
    "                doy_str = date_string[-3:]\n",
    "                doy = float(doy_str)\n",
    "                DOY_array = np.full_like(msked_target, doy)\n",
    "                doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                featureTuple += (doy_norm,)\n",
    "                featureName.append(doy)\n",
    "        \n",
    "                # get the vegetation array\n",
    "                for tree in os.listdir(vegetation_path):\n",
    "                    if tree.endswith(extension_filter):\n",
    "                        if tree.startswith(f\"{year}\"):\n",
    "                            featureName.append(f\"{tree[:-4]}\")\n",
    "                            tree_norm = read_aligned_raster(\n",
    "                            src_path=tree_workspace + tree,\n",
    "                            extent=samp_extent,\n",
    "                            target_shape=target_shape\n",
    "                            )\n",
    "                            tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                            featureTuple += (tree_norm,)\n",
    "                            if tree_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                \n",
    "        \n",
    "                # # get all the features in the fodler \n",
    "                for phv in os.listdir(phv_path):\n",
    "                    if phv.endswith(extension_filter):\n",
    "                        featureName.append(f\"{phv[:-4]}\")\n",
    "                        phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                        featureTuple += (phv_data,)\n",
    "                        if phv_data.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "                feature_stack = np.dstack(featureTuple)\n",
    "                if feature_stack.shape[2] != num_of_channels:\n",
    "                    print(f\"{sample} has shape {feature_stack.shape} â€” missing or extra feature?\")\n",
    "                    print(featureName)\n",
    "                    print(\" \")\n",
    "                else:\n",
    "                    featureArray.append(feature_stack)\n",
    "                    # targetArray.append(samp_flat)\n",
    "                    target_combined = np.concatenate([samp_flat, fsca_binary_mask])\n",
    "                    targetArray.append(target_combined)\n",
    "                    extent_list.append(samp_extent)\n",
    "                    crs_list.append(samp_crs)\n",
    "        # print(\"You go girl!\")\n",
    "        # print(\"all data split into target and feature array\")\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de30979-fc1e-479f-9560-8e72463977b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder\n",
    "print(\"Testing Group 1\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup1 = inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\"\n",
    "\n",
    "# Load test data\n",
    "X_train_g1, y_train_g1, g1_train_extents, g1_train_crs = target_feature_stacks_testGroups(\n",
    "    year=2022, \n",
    "    target_splits_path = WorkspaceBase + f\"test_groups/Group1/train/\", \n",
    "    fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "    vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "    phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "    extension_filter = \".tif\", \n",
    "    desired_shape = (256, 256), \n",
    "    debug_output_folder = \"./debug_outputs/\", \n",
    "    num_of_channels = 14\n",
    ")\n",
    "\n",
    "X_test_g1, y_test_g1, g1_test_extents, g1_test_crs = target_feature_stacks_testGroups(\n",
    "    year = 2022,\n",
    "    target_splits_path = WorkspaceBase + f\"test_groups/Group1/test/\", \n",
    "    fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "    vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "    phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "    extension_filter = \".tif\", \n",
    "    desired_shape = (256, 256), \n",
    "    debug_output_folder = \"./debug_outputs/\", \n",
    "    num_of_channels = 14\n",
    ")\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Include ALL custom functions in the custom_objects dictionary\n",
    "custom_objects = {\n",
    "    'masked_loss_fn': masked_loss_fn,\n",
    "    'masked_mse': masked_mse,\n",
    "    'masked_mae': masked_mae,\n",
    "    'masked_rmse': masked_rmse,\n",
    "    'constrained_masked_mse': constrained_masked_mse,\n",
    "    'constraint_violation_ratio': constraint_violation_ratio\n",
    "}\n",
    "\n",
    "# Load model with all custom objects\n",
    "model = load_model(\n",
    "    f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", \n",
    "    custom_objects=custom_objects\n",
    ")\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g1 = np.array(X_test_g1)\n",
    "y_test_array_g1 = np.array(y_test_g1)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g1.shape}\")  # should be (num_samples, height, width, channels)\n",
    "print(f\"y_test shape: {y_test_array_g1.shape}\")  # should confirm expected shape\n",
    "\n",
    "# Check for NaN values that could cause issues\n",
    "print(f\"X contains NaN: {np.isnan(X_test_array_g1).any()}\")\n",
    "print(f\"y contains NaN: {np.isnan(y_test_array_g1).any()}\")\n",
    "\n",
    "# Model predictions\n",
    "y_pred_g1 = model.predict(X_test_array_g1, batch_size=32)\n",
    "print(f\"Predictions shape: {y_pred_g1.shape}\")\n",
    "\n",
    "# Evaluate model\n",
    "metrics = model.evaluate(X_test_array_g1, y_test_array_g1, batch_size=32, return_dict=True)\n",
    "print(\"\\nTest Group 1 Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Save prediction images\n",
    "for i, pred in enumerate(y_pred_g1):\n",
    "    # Ensure proper reshaping based on dimensions\n",
    "    if len(pred.shape) > 2:\n",
    "        array = pred.reshape((256, 256))\n",
    "    else:\n",
    "        array = pred\n",
    "        \n",
    "    # Get correct shape for mask from y_test\n",
    "    y_test_item = y_test_g1[i]\n",
    "    if len(y_test_item.shape) > 2:\n",
    "        # Get just the SWE part if y includes both SWE and fSCA\n",
    "        half_size = 65536  # 256*256\n",
    "        if y_test_item.size > half_size:\n",
    "            y_test_item = y_test_item[:half_size].reshape((256, 256))\n",
    "        else:\n",
    "            y_test_item = y_test_item.reshape((256, 256))\n",
    "    \n",
    "    # Create mask for valid values\n",
    "    mask = y_test_item != -1\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    \n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup1}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g1_test_extents[i],\n",
    "        crs=g1_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac44b85-6468-41d3-a8ab-f64129c3fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create folder\n",
    "# print(\"Testing Group 1\")\n",
    "# os.makedirs(inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\", exist_ok=True)\n",
    "# outGroup1 = inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\"\n",
    "# X_train_g1, y_train_g1, g1_train_extents, g1_train_crs = target_feature_stacks_testGroups(year=2022, \n",
    "#                                                    target_splits_path = WorkspaceBase + f\"test_groups/Group1/train/\", \n",
    "#                                                    fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "#                                                    vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "#                                                    phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "#                                                    extension_filter = \".tif\", \n",
    "#                                                    desired_shape = (256, 256), \n",
    "#                                                    debug_output_folder = \"./debug_outputs/\", \n",
    "#                                                    num_of_channels = 14)\n",
    "# X_test_g1, y_test_g1, g1_test_extents, g1_test_crs = target_feature_stacks_testGroups(year = 2022,\n",
    "#                                                target_splits_path = WorkspaceBase + f\"test_groups/Group1/test/\", \n",
    "#                                                fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "#                                                vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "#                                                phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "#                                                extension_filter = \".tif\", \n",
    "#                                                desired_shape = (256, 256), \n",
    "#                                                debug_output_folder = \"./debug_outputs/\", \n",
    "#                                                num_of_channels = 14)\n",
    "\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Custom loss function must be re-declared if you used one\n",
    "# model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "#     'loss': masked_rmse\n",
    "# })\n",
    "\n",
    "# # Convert lists to NumPy arrays if they aren't already\n",
    "# X_test_array_g1 = np.array(X_test_g1)\n",
    "# y_test_array_g1 = np.array(y_test_g1)\n",
    "\n",
    "# # Make sure shapes are correct for model input\n",
    "# print(f\"X_test shape: {X_test_array_g1.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "# print(f\"y_test shape: {y_test_array_g1.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# # model predictions\n",
    "# y_pred_g1 = model.predict(X_test_array_g1, batch_size=32)\n",
    "\n",
    "# # test loss\n",
    "# loss = model.evaluate(X_test_array_g1, y_test_array_g1, batch_size=32)\n",
    "\n",
    "# for i, pred in enumerate(y_pred_g1):\n",
    "#     array = pred.reshape((256, 256))\n",
    "#     mask = y_test_g1[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "#     # Apply mask: set prediction to -1 where original target had no data\n",
    "#     array_masked = np.where(mask, array, -1)\n",
    "#     save_array_as_raster(\n",
    "#         output_path=f\"{outGroup1}/prediction_{i}.tif\",\n",
    "#         array=array_masked.astype(np.float32),\n",
    "#         extent=g1_test_extents[i],\n",
    "#         crs=g1_test_crs[i],\n",
    "#         nodata_val=-1\n",
    "#     )\n",
    "\n",
    "# # Print test loss and metrics\n",
    "# metrics = model.evaluate(X_test_array_g1, y_test_array_g1, batch_size=32, return_dict=True)\n",
    "# print(\"\\n Test Group 1 Metrics:\")\n",
    "# print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "# print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "# print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "# print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef485842-458d-4759-bc12-19eb6398356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"\\n Testing Group 2\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G2_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup2 = inter_model_outWorkspace + f\"outTifs_G2_yPreds_tifs/\"\n",
    "\n",
    "X_train_g2, y_train_g2, g2_train_extents, g2_train_crs = target_feature_stacks_testGroups(year=2023, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group2/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2023/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g2, y_test_g2, g2_test_extents, g2_test_crs = target_feature_stacks_testGroups(year = 2023,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group2/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2023/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g2 = np.array(X_test_g2)\n",
    "y_test_array_g2 = np.array(y_test_g2)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g2.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g2.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g2 = model.predict(X_test_array_g2, batch_size=32)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g2, y_test_array_g2, batch_size=32)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g2):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g2[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup2}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g2_test_extents[i],\n",
    "        crs=g2_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g2, y_test_array_g2, batch_size=32, return_dict=True)\n",
    "print(\"\\n Test Group 2 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973c25e-1351-4229-99b9-b37c404855d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"\\n Testing Group 3\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G3_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup3 = inter_model_outWorkspace + f\"outTifs_G3_yPreds_tifs/\"\n",
    "\n",
    "X_train_g3, y_train_g3, g3_train_extents, g3_train_crs = target_feature_stacks_testGroups(year=2024, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group3/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2024/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 14)\n",
    "X_test_g3, y_test_g3, g3_test_extents, g3_test_crs = target_feature_stacks_testGroups(year = 2024,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group3/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2024/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 14)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g3 = np.array(X_test_g3)\n",
    "y_test_array_g3 = np.array(y_test_g3)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g3.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g3.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g3 = model.predict(X_test_array_g3, batch_size=32)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g3, y_test_array_g3, batch_size=32)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g3):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g3[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup3}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g3_test_extents[i],\n",
    "        crs=g3_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g3, y_test_array_g3, batch_size=32, return_dict=True)\n",
    "\n",
    "print(\"\\n Test Group 3 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9e354-43d8-49a5-8efe-691ae5f798b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_seconds = end_time - start_time\n",
    "total_minutes = total_seconds / 60\n",
    "print(f\"Training time: {total_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac6d8a-eb1d-4b7a-bb2b-cbf7e27fa823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76c8d7-1427-4d73-bbbd-dbc4d574ee4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f5f88-52d2-4a77-b4e2-c84d7b477e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
