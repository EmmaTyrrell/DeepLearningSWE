{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb7590e-bf65-40cf-95cb-34c8e01bdd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules imported\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "# pseduo code\n",
    "import rasterio\n",
    "import shap\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "print(\"modules imported\")\n",
    "\n",
    "# file paths\n",
    "Domain = \"Sierras\"\n",
    "timestamp = \"20250508_132928\"\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/fromAlpine/\"\n",
    "savedModel = modelOuptuts + f\"{timestamp}/best_model_{timestamp}.keras\"\n",
    "shapSavedArrays = WorkspaceBase + \"SHAP_arrays/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643faf15-6cbc-4681-8760-9b9e484b7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in custom loss functions\n",
    "@register_keras_serializable()\n",
    "def masked_loss_fn(y_true, y_pred, loss_fn, mask_value=-1):\n",
    "    mask = tf.not_equal(y_true, mask_value)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return loss_fn(y_true_masked, y_pred_masked)\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mse\")\n",
    "def masked_mse(y_true, y_pred):\n",
    "    return masked_loss_fn(y_true, y_pred, MeanSquaredError())\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mae\")\n",
    "def masked_mae(y_true, y_pred):\n",
    "    return masked_loss_fn(y_true, y_pred, MeanAbsoluteError())\n",
    "\n",
    "@register_keras_serializable(name=\"masked_rmse\")\n",
    "def masked_rmse(y_true, y_pred):\n",
    "    mse = masked_loss_fn(y_true, y_pred, MeanSquaredError())\n",
    "    return tf.sqrt(mse)\n",
    "\n",
    "def constrained_mse(fsca_channel_index):\n",
    "    def loss(y_true, y_pred):\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Reshape predictions back to (batch_size, 256, 256)\n",
    "        y_pred_reshaped = tf.reshape(y_pred, (-1, 256, 256))\n",
    "\n",
    "        # Grab fSCA input from the input tensor (assumes you feed it into model.fit)\n",
    "        fsca_input = tf.keras.backend.placeholder(shape=(None, 256, 256, 18))  # youâ€™ll override this in fit\n",
    "\n",
    "        fsca = fsca_input[..., fsca_channel_index]\n",
    "\n",
    "        # Create a mask for where fSCA > 0 but y_pred <= 0\n",
    "        violation_mask = tf.logical_and(fsca > 0, y_pred_reshaped <= 0)\n",
    "        violation_penalty = tf.cast(violation_mask, tf.float32) * 100.0  # 100 is an arbitrary penalty factor\n",
    "\n",
    "        penalty = tf.reduce_mean(violation_penalty, axis=[1, 2])  # penalize across spatial dimensions\n",
    "        return mse + penalty\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3468d85e-f82f-45f8-bd04-a63c59f06f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model, arrays, and list\n",
    "model = tf.keras.models.load_model(savedModel)\n",
    "\n",
    "# Load arrays\n",
    "X_train = np.load(f'{shapSavedArrays}/X_train.npy')\n",
    "X_valid = np.load(f'{shapSavedArrays}/X_valid.npy')\n",
    "# Load other arrays if needed\n",
    "y_train = np.load(f'{shapSavedArrays}/y_train.npy')\n",
    "y_valid = np.load(f'{shapSavedArrays}/y_valid.npy')\n",
    "\n",
    "# Load featureName list\n",
    "with open(f'{shapSavedArrays}/featureName.pkl', 'rb') as f:\n",
    "    featureName = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75e43be-f938-4f74-ade6-a336f0de8145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP: running SHAP\n",
      "####background data established\n",
      "####explainer established\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etyrr\\anaconda3\\envs\\aso-dl\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 900. MiB for an array with shape (200, 256, 256, 18) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m####explainer established\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m examples_to_explain \u001b[38;5;241m=\u001b[39m X_valid\n\u001b[1;32m----> 9\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples_to_explain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m####shap values determined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate mean absolute SHAP value for each channel\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aso-dl\\lib\\site-packages\\shap\\explainers\\_gradient.py:158\u001b[0m, in \u001b[0;36mGradientExplainer.shap_values\u001b[1;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, nsamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, rseed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_variances\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the values for the model applied to X.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_variances\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aso-dl\\lib\\site-packages\\shap\\explainers\\_gradient.py:341\u001b[0m, in \u001b[0;36m_TFGradient.shap_values\u001b[1;34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[0m\n\u001b[0;32m    339\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [samples_input[a][b:\u001b[38;5;28mmin\u001b[39m(b\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,nsamples)] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))]\n\u001b[0;32m    340\u001b[0m     grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient(find), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs, batch))\n\u001b[1;32m--> 341\u001b[0m grad \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mconcatenate([g[a] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads], \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aso-dl\\lib\\site-packages\\shap\\explainers\\_gradient.py:341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    339\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [samples_input[a][b:\u001b[38;5;28mmin\u001b[39m(b\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,nsamples)] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))]\n\u001b[0;32m    340\u001b[0m     grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient(find), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs, batch))\n\u001b[1;32m--> 341\u001b[0m grad \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X)):\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 900. MiB for an array with shape (200, 256, 256, 18) and data type float32"
     ]
    }
   ],
   "source": [
    "print(\"SHAP: running SHAP\")\n",
    "        \n",
    "# Steps 1-4 as in the previous code\n",
    "background_data = X_train\n",
    "print(\"####background data established\")\n",
    "explainer = shap.GradientExplainer(model, background_data)\n",
    "print(\"####explainer established\")\n",
    "examples_to_explain = X_valid\n",
    "shap_values = explainer.shap_values(examples_to_explain)\n",
    "print(\"####shap values determined\")\n",
    "\n",
    "# Calculate mean absolute SHAP value for each channel\n",
    "channel_importance = np.abs(shap_values[0]).mean(axis=(0, 1, 2))\n",
    "print(\"####calculating channel importances\")\n",
    "\n",
    "# Create channel importance plot with feature names\n",
    "plt.figure(figsize=(14, 7))\n",
    "bars = plt.bar(range(18), channel_importance)\n",
    "plt.xlabel('Channels', fontsize=12)\n",
    "plt.ylabel('Mean |SHAP value|', fontsize=12)\n",
    "plt.title('Channel Importance by Feature', fontsize=14)\n",
    "print(\"####channel importance created\")\n",
    "\n",
    "# Use the feature names as x-tick labels\n",
    "plt.xticks(range(18), featureName, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Add values on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "             f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(shapSavedArrays + 'channel_importance_with_features.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# You can also create a sorted version to see the most important features first\n",
    "sorted_idx = np.argsort(channel_importance)[::-1]  \n",
    "sorted_importance = channel_importance[sorted_idx]\n",
    "sorted_features = [featureName[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "bars = plt.bar(range(18), sorted_importance)\n",
    "plt.xlabel('Channels', fontsize=12)\n",
    "plt.ylabel('Mean |SHAP value|', fontsize=12)\n",
    "plt.title('Channel Importance by Feature (Sorted)', fontsize=14)\n",
    "plt.xticks(range(18), sorted_features, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Add values on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "             f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(shapSavedArrays + 'sorted_channel_importance.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452a8e1-a76b-4955-b7a8-5a593b342e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
