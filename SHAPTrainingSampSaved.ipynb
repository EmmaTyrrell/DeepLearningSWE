{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0520572c-7d3d-49d5-9a60-877d0384e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules established\n",
      "MODEL OUTPUTS TO BE PRINTED TO THIS DOC\n"
     ]
    }
   ],
   "source": [
    "# pseduo code\n",
    "import rasterio\n",
    "import shap\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "years = list(range(2022, 2025))\n",
    "Domain = \"Sierras\"\n",
    "SHAP = \"N\"\n",
    "\n",
    "# workspaces\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "land_workspace = WorkspaceBase + \"landCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "shapSavedArrays = WorkspaceBase + \"SHAP_arrays/\"\n",
    "# final_activation = 'relu'\n",
    "\n",
    "## seting folder\n",
    "# from datetime import datetime\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# create folder for model outputs\n",
    "# os.makedirs(modelOuptuts + f\"{str(timestamp)}/\", exist_ok=True)\n",
    "# inter_model_outWorkspace = modelOuptuts + f\"{str(timestamp)}/\"\n",
    "\n",
    "# f = open(inter_model_outWorkspace + f\"code_output_{timestamp}.txt\", \"a\")\n",
    "# sys.stdout = f\n",
    "print(\"MODEL OUTPUTS TO BE PRINTED TO THIS DOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc9f26e-9221-4c46-a689-ad61312bda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for min-max scaling\n",
    "def min_max_scale(data, min_val=None, max_val=None, feature_range=(0, 1)):\n",
    "    \"\"\"Min-Max normalize a NumPy array to a target range.\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    mask = np.isnan(data)\n",
    "\n",
    "    d_min = np.nanmin(data) if min_val is None else min_val\n",
    "    d_max = np.nanmax(data) if max_val is None else max_val\n",
    "\n",
    "    # if d_max == d_min:\n",
    "    #     raise ValueError(\"Min and max are equal — can't scale.\")\n",
    "    if d_max == d_min:\n",
    "        return np.full_like(data, feature_range[0], dtype=np.float32)\n",
    "\n",
    "    a, b = feature_range\n",
    "    scaled = (data - d_min) / (d_max - d_min)  # to [0, 1]\n",
    "    scaled = scaled * (b - a) + a              # to [a, b]\n",
    "\n",
    "    scaled[mask] = np.nan  # preserve NaNs\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07ce437-9d37-4dd2-87d8-cdca9d00825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(X):\n",
    "    \"\"\"\n",
    "    Wrapper function to get predictions from the model.\n",
    "    For CNNs with spatial outputs, you might want to either:\n",
    "    1. Focus on one specific pixel location or\n",
    "    2. Average across all spatial dimensions\n",
    "    \"\"\"\n",
    "    preds = model.predict(X)\n",
    "    # For a model with many output pixels (65536 in your case), you might want to:\n",
    "    # - Either focus on specific pixels\n",
    "    # - Or aggregate across all pixels (e.g., mean)\n",
    "    return preds.reshape(X.shape[0], -1)  # Reshape to (batch_size, all_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8edd19-1da4-4f73-899c-06666a947982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aligned_raster(src_path, extent, target_shape, nodata_val=-1):\n",
    "    height, width = target_shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        try:\n",
    "            data = src.read(\n",
    "                1,\n",
    "                out_shape=target_shape,\n",
    "                resampling=rasterio.enums.Resampling.nearest,\n",
    "                window=src.window(*extent)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {src_path}: {e}\")\n",
    "            return np.full(target_shape, nodata_val, dtype=np.float32)\n",
    "\n",
    "        # Handle nodata in source\n",
    "        src_nodata = src.nodata\n",
    "        if src_nodata is not None:\n",
    "            data = np.where(data == src_nodata, np.nan, data)\n",
    "\n",
    "        # Replace NaNs or invalid with -1\n",
    "        data = np.where(np.isnan(data), nodata_val, data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28eda428-53c0-48f1-b59d-07d57c4bee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array_as_raster(output_path, array, extent, crs, nodata_val=-1):\n",
    "    height, width = array.shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=array.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "        nodata=nodata_val\n",
    "    ) as dst:\n",
    "        dst.write(array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e752cb53-24fe-423c-82b6-99ceab70f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2022\n",
      "Processing year 2023\n",
      "Processing year 2024\n"
     ]
    }
   ],
   "source": [
    "# split up the features and arrarys \n",
    "## create empty arrays\n",
    "featureArray = []\n",
    "targetArray = []\n",
    "\n",
    "# loop through the years and feature data\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}\")\n",
    "    targetSplits = WorkspaceBase + f\"{year}/SHAPsamp/\"\n",
    "    fSCAWorkspace = WorkspaceBase + f\"{year}/fSCA/\"\n",
    "    for sample in os.listdir(targetSplits):\n",
    "        featureTuple = ()\n",
    "        featureName = []\n",
    "        # loop through each sample and get the corresponding features\n",
    "        if sample.endswith(\"nonull_fnl.tif\"):\n",
    "            # read in data\n",
    "            with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                samp_data = samp_src.read(1)\n",
    "                meta = samp_src.meta.copy()\n",
    "                samp_extent = samp_src.bounds\n",
    "                samp_transform = samp_src.transform\n",
    "                samp_crs = samp_src.crs\n",
    "    \n",
    "                # apply a mask to all no data values. Reminder that nodata values is -9999\n",
    "                mask = samp_data >= 0\n",
    "                msked_target = np.where(mask, samp_data, -1)\n",
    "                target_shape = msked_target.shape\n",
    "    \n",
    "                # flatted data\n",
    "                samp_flat = msked_target.flatten()\n",
    "                \n",
    "\n",
    "            # try to get the fsca variables \n",
    "            sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "            for fSCA in os.listdir(fSCAWorkspace):\n",
    "                if fSCA.endswith(\".tif\") and fSCA.startswith(sample_root):\n",
    "                    # featureName.append(f\"fSCA\")\n",
    "                    featureName.append(f\"fSCA\")\n",
    "                    fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                    fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                    featureTuple += (fsca_norm,)\n",
    "                    # print(fsca_norm.shape)\n",
    "                    if fsca_norm.shape != (256, 256):\n",
    "                        print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                        output_debug_path = f\"./debug_output/{sample_root}_BAD_FSCA.tif\"\n",
    "                        save_array_as_raster(\n",
    "                            output_path=output_debug_path,\n",
    "                            array=fsca_norm,\n",
    "                            extent=samp_extent,\n",
    "                            crs=samp_crs,\n",
    "                            nodata_val=-1\n",
    "                        )\n",
    "    \n",
    "            # get a DOY array into a feature \n",
    "            date_string = sample.split(\"_\")[1]\n",
    "            doy_str = date_string[-3:]\n",
    "            doy = float(doy_str)\n",
    "            DOY_array = np.full_like(msked_target, doy)\n",
    "            doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "            featureTuple += (doy_norm,)\n",
    "            featureName.append(\"DOY\")\n",
    "    \n",
    "            # get the vegetation array\n",
    "            for tree in os.listdir(tree_workspace):\n",
    "                if tree.endswith(\".tif\"):\n",
    "                    if tree.startswith(f\"{year}\"):\n",
    "                        # featureName.append(f\"{tree[:-4]}\")\n",
    "                        featureName.append(f\"Tree Density\")\n",
    "                        tree_norm = read_aligned_raster(\n",
    "                        src_path=tree_workspace + tree,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (tree_norm,)\n",
    "                        if tree_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                            output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=tree_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            # get the vegetation array\n",
    "            for land in os.listdir(land_workspace):\n",
    "                if land.endswith(\".tif\"):\n",
    "                    if land.startswith(f\"{year}\"):\n",
    "                        # featureName.append(f\"{tree[:-4]}\")\n",
    "                        featureName.append(f\"LandCover\")\n",
    "                        land_norm = read_aligned_raster(\n",
    "                        src_path=land_workspace + land,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        land_norm = min_max_scale(land_norm, min_val=11, max_val=95)\n",
    "                        featureTuple += (land_norm,)\n",
    "                        if land_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: Land\")\n",
    "                            # output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=land_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            \n",
    "            # # get all the features in the fodler \n",
    "            for phv in os.listdir(phv_features):\n",
    "                if phv.endswith(\".tif\"):\n",
    "                    featureName.append(f\"{phv[:-4]}\")\n",
    "                    phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                    featureTuple += (phv_data,)\n",
    "                    if phv_data.shape != (256, 256):\n",
    "                         print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                        \n",
    "            feature_stack = np.dstack(featureTuple)\n",
    "            if feature_stack.shape[2] != 18:\n",
    "                print(f\"{sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                print(featureName)\n",
    "                print(\" \")\n",
    "            else:\n",
    "                featureArray.append(feature_stack)\n",
    "                targetArray.append(samp_flat)\n",
    "X = np.array(featureArray)\n",
    "y = np.array(targetArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32cef5c-d24b-4b5d-a20c-9de3093f27b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of input data\n",
      "feature shape: (18, 256, 256, 18)\n",
      "target shape: (18, 65536)\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"Shape of input data\")\n",
    "print(f\"feature shape: {X.shape}\")\n",
    "print(f\"target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33eef0a0-d3a5-4f13-ba41-aa2867657830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "________________________________ Training and Validation Data Shapes ________________________________\n",
      "Training data shape: (15, 256, 256, 18) (15, 65536)\n",
      "Validation data shape: (3, 256, 256, 18) (3, 65536)\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "# split between training and test data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "print(\"***\")\n",
    "print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "print(\"***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618f1c82-9398-4fa0-a21f-440c89167397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save arrays to load\n",
    "np.save(f'{shapSavedArrays}/X_train.npy', X_train)\n",
    "np.save(f'{shapSavedArrays}/X_valid.npy', X_valid)\n",
    "# If you need to save other arrays like y_train, y_valid:\n",
    "np.save(f'{shapSavedArrays}/y_train.npy', y_train)\n",
    "np.save(f'{shapSavedArrays}/y_valid.npy', y_valid)\n",
    "\n",
    "# You might also want to save featureName list if it's needed for SHAP visualization\n",
    "import pickle\n",
    "with open(f'{shapSavedArrays}/featureName.pkl', 'wb') as f:\n",
    "    pickle.dump(featureName, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c553070-005e-4326-916d-f5fd4b08d40d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
