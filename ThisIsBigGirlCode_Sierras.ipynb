{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ac281b-3e91-4fb8-a7c8-d63f4ba8d0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules established\n"
     ]
    }
   ],
   "source": [
    "# pseduo code\n",
    "import rasterio\n",
    "import shap\n",
    "from rasterio.mask import mask\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.transform import from_bounds \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Input, Add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.transform import from_bounds\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"modules established\")\n",
    "\n",
    "## establish file paths\n",
    "years = list(range(2024, 2025))\n",
    "Domain = \"Sierras\"\n",
    "SHAP = \"N\"\n",
    "\n",
    "# workspaces\n",
    "WorkspaceBase = f\"D:/ASOML/{Domain}/\"\n",
    "phv_features = WorkspaceBase + \"features/scaled/\"\n",
    "tree_workspace = WorkspaceBase + \"treeCover/\"\n",
    "land_workspace = WorkspaceBase + \"landCover/\"\n",
    "modelOuptuts = WorkspaceBase + \"modelOutputs/\"\n",
    "final_activation = 'relu'\n",
    "\n",
    "## seting folder\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# create folder for model outputs\n",
    "os.makedirs(modelOuptuts + f\"{str(timestamp)}/\", exist_ok=True)\n",
    "inter_model_outWorkspace = modelOuptuts + f\"{str(timestamp)}/\"\n",
    "\n",
    "f = open(inter_model_outWorkspace + f\"code_output_{timestamp}.txt\", \"a\")\n",
    "sys.stdout = f\n",
    "print(\"MODEL OUTPUTS TO BE PRINTED TO THIS DOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb691092-ac56-40fd-9520-408e45a1a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for min-max scaling\n",
    "def min_max_scale(data, min_val=None, max_val=None, feature_range=(0, 1)):\n",
    "    \"\"\"Min-Max normalize a NumPy array to a target range.\"\"\"\n",
    "    data = data.astype(np.float32)\n",
    "    mask = np.isnan(data)\n",
    "\n",
    "    d_min = np.nanmin(data) if min_val is None else min_val\n",
    "    d_max = np.nanmax(data) if max_val is None else max_val\n",
    "\n",
    "    # if d_max == d_min:\n",
    "    #     raise ValueError(\"Min and max are equal — can't scale.\")\n",
    "    if d_max == d_min:\n",
    "        return np.full_like(data, feature_range[0], dtype=np.float32)\n",
    "\n",
    "    a, b = feature_range\n",
    "    scaled = (data - d_min) / (d_max - d_min)  # to [0, 1]\n",
    "    scaled = scaled * (b - a) + a              # to [a, b]\n",
    "\n",
    "    scaled[mask] = np.nan  # preserve NaNs\n",
    "    return scaled\n",
    "\n",
    "def model_predict(X):\n",
    "    \"\"\"\n",
    "    Wrapper function to get predictions from the model.\n",
    "    For CNNs with spatial outputs, you might want to either:\n",
    "    1. Focus on one specific pixel location or\n",
    "    2. Average across all spatial dimensions\n",
    "    \"\"\"\n",
    "    preds = model.predict(X)\n",
    "    # For a model with many output pixels (65536 in your case), you might want to:\n",
    "    # - Either focus on specific pixels\n",
    "    # - Or aggregate across all pixels (e.g., mean)\n",
    "    return preds.reshape(X.shape[0], -1)  # Reshape to (batch_size, all_pixels)\n",
    "\n",
    "def read_aligned_raster(src_path, extent, target_shape, nodata_val=-1):\n",
    "    height, width = target_shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "\n",
    "    with rasterio.open(src_path) as src:\n",
    "        try:\n",
    "            data = src.read(\n",
    "                1,\n",
    "                out_shape=target_shape,\n",
    "                resampling=rasterio.enums.Resampling.nearest,\n",
    "                window=src.window(*extent)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {src_path}: {e}\")\n",
    "            return np.full(target_shape, nodata_val, dtype=np.float32)\n",
    "\n",
    "        # Handle nodata in source\n",
    "        src_nodata = src.nodata\n",
    "        if src_nodata is not None:\n",
    "            data = np.where(data == src_nodata, np.nan, data)\n",
    "\n",
    "        # Replace NaNs or invalid with -1\n",
    "        data = np.where(np.isnan(data), nodata_val, data)\n",
    "\n",
    "        return data\n",
    "\n",
    "def save_array_as_raster(output_path, array, extent, crs, nodata_val=-1):\n",
    "    height, width = array.shape\n",
    "    transform = from_bounds(*extent, width=width, height=height)\n",
    "    \n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=height,\n",
    "        width=width,\n",
    "        count=1,\n",
    "        dtype=array.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform,\n",
    "        nodata=nodata_val\n",
    "    ) as dst:\n",
    "        dst.write(array, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57c3d3-fd37-4067-a32b-4d53fb69db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up the features and arrarys \n",
    "## create empty arrays\n",
    "featureArray = []\n",
    "targetArray = []\n",
    "\n",
    "# loop through the years and feature data\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}\")\n",
    "    targetSplits = WorkspaceBase + f\"{year}/SWE_processed_splits/\"\n",
    "    fSCAWorkspace = WorkspaceBase + f\"{year}/fSCA/\"\n",
    "    for sample in os.listdir(targetSplits):\n",
    "        featureTuple = ()\n",
    "        featureName = []\n",
    "        # loop through each sample and get the corresponding features\n",
    "        if sample.endswith(\"nonull_fnl.tif\"):\n",
    "            # read in data\n",
    "            with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                samp_data = samp_src.read(1)\n",
    "                meta = samp_src.meta.copy()\n",
    "                samp_extent = samp_src.bounds\n",
    "                samp_transform = samp_src.transform\n",
    "                samp_crs = samp_src.crs\n",
    "    \n",
    "                # apply a mask to all no data values. Reminder that nodata values is -9999\n",
    "                mask = samp_data >= 0\n",
    "                msked_target = np.where(mask, samp_data, -1)\n",
    "                target_shape = msked_target.shape\n",
    "    \n",
    "                # flatted data\n",
    "                samp_flat = msked_target.flatten()\n",
    "                \n",
    "\n",
    "            # try to get the fsca variables \n",
    "            sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "            for fSCA in os.listdir(fSCAWorkspace):\n",
    "                if fSCA.endswith(\".tif\") and fSCA.startswith(sample_root):\n",
    "                    # featureName.append(f\"fSCA\")\n",
    "                    featureName.append(f\"fSCA\")\n",
    "                    fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                    fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                    featureTuple += (fsca_norm,)\n",
    "                    # print(fsca_norm.shape)\n",
    "                    if fsca_norm.shape != (256, 256):\n",
    "                        print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                        output_debug_path = f\"./debug_output/{sample_root}_BAD_FSCA.tif\"\n",
    "                        save_array_as_raster(\n",
    "                            output_path=output_debug_path,\n",
    "                            array=fsca_norm,\n",
    "                            extent=samp_extent,\n",
    "                            crs=samp_crs,\n",
    "                            nodata_val=-1\n",
    "                        )\n",
    "    \n",
    "            # get a DOY array into a feature \n",
    "            date_string = sample.split(\"_\")[1]\n",
    "            doy_str = date_string[-3:]\n",
    "            doy = float(doy_str)\n",
    "            DOY_array = np.full_like(msked_target, doy)\n",
    "            doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "            featureTuple += (doy_norm,)\n",
    "            featureName.append(\"DOY\")\n",
    "    \n",
    "            # get the vegetation array\n",
    "            for tree in os.listdir(tree_workspace):\n",
    "                if tree.endswith(\".tif\"):\n",
    "                    if tree.startswith(f\"{year}\"):\n",
    "                        # featureName.append(f\"{tree[:-4]}\")\n",
    "                        featureName.append(f\"Tree Density\")\n",
    "                        tree_norm = read_aligned_raster(\n",
    "                        src_path=tree_workspace + tree,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (tree_norm,)\n",
    "                        if tree_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                            output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            # get the vegetation array\n",
    "            for land in os.listdir(land_workspace):\n",
    "                if land.endswith(\".tif\"):\n",
    "                    if land.startswith(f\"{year}\"):\n",
    "                        # featureName.append(f\"{tree[:-4]}\")\n",
    "                        featureName.append(f\"LandCover\")\n",
    "                        land_norm = read_aligned_raster(\n",
    "                        src_path=land_workspace + land,\n",
    "                        extent=samp_extent,\n",
    "                        target_shape=target_shape\n",
    "                        )\n",
    "                        land_norm = min_max_scale(land_norm, min_val=11, max_val=95)\n",
    "                        featureTuple += (land_norm,)\n",
    "                        if land_norm.shape != (256, 256):\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: Land\")\n",
    "                            # output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "            \n",
    "            # # get all the features in the fodler \n",
    "            for phv in os.listdir(phv_features):\n",
    "                if phv.endswith(\".tif\"):\n",
    "                    featureName.append(f\"{phv[:-4]}\")\n",
    "                    phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                    featureTuple += (phv_data,)\n",
    "                    if phv_data.shape != (256, 256):\n",
    "                         print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                        \n",
    "            feature_stack = np.dstack(featureTuple)\n",
    "            if feature_stack.shape[2] != 18:\n",
    "                print(f\"{sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                print(featureName)\n",
    "                print(\" \")\n",
    "            else:\n",
    "                featureArray.append(feature_stack)\n",
    "                targetArray.append(samp_flat)\n",
    "X = np.array(featureArray)\n",
    "y = np.array(targetArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1c4fb-ed92-4b3a-8151-dee004c79665",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(\"Shape of input data\")\n",
    "print(f\"feature shape: {X.shape}\")\n",
    "print(f\"target shape: {y.shape}\")\n",
    "\n",
    "# split between training and test data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "print(\"***\")\n",
    "print(\"________________________________ Training and Validation Data Shapes ________________________________\")\n",
    "print(\"Training data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape:\", X_valid.shape, y_valid.shape)\n",
    "print(\"***\")\n",
    "\n",
    "# code model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(256, 256, 18), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "# add stacked convolutions\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# dense layer\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "# output layer\n",
    "model.add(Dense(65536, activation=final_activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8bbec-3abe-4a99-848b-e28fad38ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "def masked_loss_fn(y_true, y_pred, loss_fn, mask_value=-1):\n",
    "    mask = tf.not_equal(y_true, mask_value)\n",
    "    y_true_masked = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_masked = tf.boolean_mask(y_pred, mask)\n",
    "    return loss_fn(y_true_masked, y_pred_masked)\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mse\")\n",
    "def masked_mse(y_true, y_pred):\n",
    "    return masked_loss_fn(y_true, y_pred, MeanSquaredError())\n",
    "\n",
    "@register_keras_serializable(name=\"masked_mae\")\n",
    "def masked_mae(y_true, y_pred):\n",
    "    return masked_loss_fn(y_true, y_pred, MeanAbsoluteError())\n",
    "\n",
    "@register_keras_serializable(name=\"masked_rmse\")\n",
    "def masked_rmse(y_true, y_pred):\n",
    "    mse = masked_loss_fn(y_true, y_pred, MeanSquaredError())\n",
    "    return tf.sqrt(mse)\n",
    "\n",
    "def constrained_mse(fsca_channel_index):\n",
    "    def loss(y_true, y_pred):\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Reshape predictions back to (batch_size, 256, 256)\n",
    "        y_pred_reshaped = tf.reshape(y_pred, (-1, 256, 256))\n",
    "\n",
    "        # Grab fSCA input from the input tensor (assumes you feed it into model.fit)\n",
    "        fsca_input = tf.keras.backend.placeholder(shape=(None, 256, 256, 18))  # you’ll override this in fit\n",
    "\n",
    "        fsca = fsca_input[..., fsca_channel_index]\n",
    "\n",
    "        # Create a mask for where fSCA > 0 but y_pred <= 0\n",
    "        violation_mask = tf.logical_and(fsca > 0, y_pred_reshaped <= 0)\n",
    "        violation_penalty = tf.cast(violation_mask, tf.float32) * 100.0  # 100 is an arbitrary penalty factor\n",
    "\n",
    "        penalty = tf.reduce_mean(violation_penalty, axis=[1, 2])  # penalize across spatial dimensions\n",
    "        return mse + penalty\n",
    "    return loss\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Fetch batch data\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "\n",
    "        # Optionally, do preprocessing or masking here\n",
    "        # (e.g., ensure y_batch is ready for custom loss with -1 masking)\n",
    "        y_batch[y_batch == -1] = 0\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7d4ab-d763-47c5-8ffd-6f8f37953ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading and saving model stats\n",
    "checkpoint = ModelCheckpoint(\n",
    "    f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", monitor=\"val_masked_rmse\",\n",
    "    verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor=\"val_masked_rmse\", mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "\n",
    "#compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=masked_mse,  # function version, already mask-aware\n",
    "    metrics=[masked_rmse, masked_mae, masked_mse]\n",
    ")\n",
    "\n",
    "# get model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb25ac2-5780-4744-b7c4-b1e74f37ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish the model\n",
    "batch_size = 32\n",
    "train_generator = DataGenerator(X_train, y_train, batch_size=batch_size)\n",
    "valid_generator = DataGenerator(X_valid, y_valid, batch_size=batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450a356-9111-492d-b476-1554b2585be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP == \"Y\":\n",
    "    try:\n",
    "        print(\"SHAP: running SHAP\")\n",
    "        \n",
    "        # Steps 1-4 as in the previous code\n",
    "        background_data = X_train[:20]\n",
    "        print(\"####background data established\")\n",
    "        explainer = shap.GradientExplainer(model, background_data)\n",
    "        print(\"####explainer established\")\n",
    "        examples_to_explain = X_valid[:10]\n",
    "        shap_values = explainer.shap_values(examples_to_explain)\n",
    "        print(\"####shap values determined\")\n",
    "        \n",
    "        # Calculate mean absolute SHAP value for each channel\n",
    "        channel_importance = np.abs(shap_values[0]).mean(axis=(0, 1, 2))\n",
    "        print(\"####calculating channel importances\")\n",
    "        \n",
    "        # Create channel importance plot with feature names\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        bars = plt.bar(range(18), channel_importance)\n",
    "        plt.xlabel('Channels', fontsize=12)\n",
    "        plt.ylabel('Mean |SHAP value|', fontsize=12)\n",
    "        plt.title('Channel Importance by Feature', fontsize=14)\n",
    "        print(\"####channel importance created\")\n",
    "        \n",
    "        # Use the feature names as x-tick labels\n",
    "        plt.xticks(range(18), featureName, rotation=45, ha='right', fontsize=10)\n",
    "        \n",
    "        # Add values on top of each bar\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "                     f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(inter_model_outWorkspace + 'channel_importance_with_features.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # You can also create a sorted version to see the most important features first\n",
    "        sorted_idx = np.argsort(channel_importance)[::-1]  \n",
    "        sorted_importance = channel_importance[sorted_idx]\n",
    "        sorted_features = [featureName[i] for i in sorted_idx]\n",
    "        \n",
    "        plt.figure(figsize=(14, 7))\n",
    "        bars = plt.bar(range(18), sorted_importance)\n",
    "        plt.xlabel('Channels', fontsize=12)\n",
    "        plt.ylabel('Mean |SHAP value|', fontsize=12)\n",
    "        plt.title('Channel Importance by Feature (Sorted)', fontsize=14)\n",
    "        plt.xticks(range(18), sorted_features, rotation=45, ha='right', fontsize=10)\n",
    "        \n",
    "        # Add values on top of each bar\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "                     f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(inter_model_outWorkspace + 'sorted_channel_importance.png', dpi=300)\n",
    "        plt.show()\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "else:\n",
    "    print(\"SHAP was not run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33daad-ddc0-4900-a149-78481ee6aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# 1. Masked MSE (used as loss)\n",
    "axs[0].plot(history.history['loss'], label='Train MSE Loss')\n",
    "axs[0].plot(history.history['val_loss'], label='Val MSE Loss')\n",
    "axs[0].set_ylabel('MSE Loss')\n",
    "axs[0].set_title('Masked MSE')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# 2. Masked RMSE\n",
    "axs[1].plot(history.history['masked_rmse'], label='Train RMSE')\n",
    "axs[1].plot(history.history['val_masked_rmse'], label='Val RMSE')\n",
    "axs[1].set_ylabel('RMSE')\n",
    "axs[1].set_title('Masked RMSE')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# 3. Masked MAE\n",
    "axs[2].plot(history.history['masked_mae'], label='Train MAE')\n",
    "axs[2].plot(history.history['val_masked_mae'], label='Val MAE')\n",
    "axs[2].set_ylabel('MAE')\n",
    "axs[2].set_title('Masked MAE')\n",
    "axs[2].set_xlabel('Epoch')\n",
    "axs[2].legend()\n",
    "axs[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(inter_model_outWorkspace + \"Model_error_epochs.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cf1c1-e07d-4343-b27d-736766475e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_track = ['val_masked_rmse', 'val_masked_mse', 'val_masked_mae']\n",
    "\n",
    "print(\"\\nValidation Metric Progression:\")\n",
    "for metric in metrics_to_track:\n",
    "    values = history.history.get(metric, [])\n",
    "    if values:\n",
    "        print(f\"{metric}: Start = {values[0]:.4f}, End = {values[-1]:.4f}\")\n",
    "        print(f\"{metric}: Best = {min(values):.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric}: Not found in history.\")\n",
    "\n",
    "print(f\"Final activation function: {final_activation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e27f96-8b01-4128-be68-baa4fa2460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks(start_year, end_year, target_splits_path, fSCA_path, vegetation_path, landCover_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        years = list(range(start_year, (int(end_year) + 1)))\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        for year in years:\n",
    "            print(f\"Processing {year}\")\n",
    "            targetSplits = target_splits_path\n",
    "            fSCAWorkspace = fSCA_path\n",
    "            for sample in os.listdir(targetSplits):\n",
    "                featureTuple = ()\n",
    "                featureName = []\n",
    "                # loop through each sample and get the corresponding features\n",
    "                if sample.endswith(extension_filter):\n",
    "                    # read in data\n",
    "                    with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                        samp_data = samp_src.read(1)\n",
    "                        meta = samp_src.meta.copy()\n",
    "                        samp_extent = samp_src.bounds\n",
    "                        samp_transform = samp_src.transform\n",
    "                        samp_crs = samp_src.crs\n",
    "                        # apply a no-data mask\n",
    "                        mask = samp_data >= 0\n",
    "                        msked_target = np.where(mask, samp_data, -1)\n",
    "                        target_shape = msked_target.shape\n",
    "            \n",
    "                        # flatted data\n",
    "                        samp_flat = msked_target.flatten()\n",
    "                        \n",
    "        \n",
    "                    # try to get the fsca variables \n",
    "                    sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                    for fSCA in os.listdir(fSCAWorkspace):\n",
    "                        if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                            featureName.append(f\"{fSCA[:-4]}\")\n",
    "                            fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                            fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                            featureTuple += (fsca_norm,)\n",
    "                            # print(fsca_norm.shape)\n",
    "                            if fsca_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "            \n",
    "                    # get a DOY array into a feature \n",
    "                    date_string = sample.split(\"_\")[1]\n",
    "                    doy_str = date_string[-3:]\n",
    "                    doy = float(doy_str)\n",
    "                    DOY_array = np.full_like(msked_target, doy)\n",
    "                    doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                    featureTuple += (doy_norm,)\n",
    "                    featureName.append(doy)\n",
    "            \n",
    "                    # get the vegetation array\n",
    "                    for tree in os.listdir(vegetation_path):\n",
    "                        if tree.endswith(extension_filter):\n",
    "                            if tree.startswith(f\"{year}\"):\n",
    "                                featureName.append(f\"{tree[:-4]}\")\n",
    "                                tree_norm = read_aligned_raster(\n",
    "                                src_path=tree_workspace + tree,\n",
    "                                extent=samp_extent,\n",
    "                                target_shape=target_shape\n",
    "                                )\n",
    "                                tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                                featureTuple += (tree_norm,)\n",
    "                                if tree_norm.shape != desired_shape:\n",
    "                                    print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                    output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                    save_array_as_raster(\n",
    "                                        output_path=output_debug_path,\n",
    "                                        array=fsca_norm,\n",
    "                                        extent=samp_extent,\n",
    "                                        crs=samp_crs,\n",
    "                                        nodata_val=-1\n",
    "                                    )\n",
    "                    \n",
    "                    # get the vegetation array\n",
    "                    for land in os.listdir(land_workspace):\n",
    "                        if land.endswith(\".tif\"):\n",
    "                            if land.startswith(f\"{year}\"):\n",
    "                                # featureName.append(f\"{tree[:-4]}\")\n",
    "                                featureName.append(f\"LandCover\")\n",
    "                                land_norm = read_aligned_raster(\n",
    "                                src_path=tree_workspace + land,\n",
    "                                extent=samp_extent,\n",
    "                                target_shape=target_shape\n",
    "                                )\n",
    "                                land_norm = min_max_scale(land_norm, min_val=11, max_val=95)\n",
    "                                featureTuple += (land_norm,)\n",
    "                                if land_norm.shape != (256, 256):\n",
    "                                    print(f\"WRONG SHAPE FOR {sample}: Land\")\n",
    "                                    # output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                                    save_array_as_raster(\n",
    "                                        output_path=output_debug_path,\n",
    "                                        array=fsca_norm,\n",
    "                                        extent=samp_extent,\n",
    "                                        crs=samp_crs,\n",
    "                                        nodata_val=-1\n",
    "                                    )\n",
    "                    \n",
    "            \n",
    "                    # # get all the features in the fodler \n",
    "                    for phv in os.listdir(phv_path):\n",
    "                        if phv.endswith(extension_filter):\n",
    "                            featureName.append(f\"{phv[:-4]}\")\n",
    "                            phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                            featureTuple += (phv_data,)\n",
    "                            if phv_data.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                    feature_stack = np.dstack(featureTuple)\n",
    "                    if feature_stack.shape[2] != num_of_channels:\n",
    "                        print(f\"{sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                        print(featureName)\n",
    "                        print(\" \")\n",
    "                    else:\n",
    "                        featureArray.append(feature_stack)\n",
    "                        targetArray.append(samp_flat)\n",
    "                        extent_list.append(samp_extent)\n",
    "                        crs_list.append(samp_crs)\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1b35e-4777-4532-a516-3022e3a440e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing out this function with test date\n",
    "# split up the features and arrarys \n",
    "def target_feature_stacks_testGroups(year, target_splits_path, fSCA_path, vegetation_path, landCover_path, phv_path, extension_filter, desired_shape, debug_output_folder, num_of_channels):\n",
    "        ## create empty arrays\n",
    "        featureArray = []\n",
    "        targetArray = []\n",
    "        extent_list = []\n",
    "        crs_list = []\n",
    "        \n",
    "        # loop through the years and feature data\n",
    "        # print(f\"Processing {group}\")\n",
    "        targetSplits = target_splits_path\n",
    "        fSCAWorkspace = fSCA_path\n",
    "        for sample in os.listdir(targetSplits):\n",
    "            featureTuple = ()\n",
    "            featureName = []\n",
    "            # loop through each sample and get the corresponding features\n",
    "            if sample.endswith(extension_filter):\n",
    "                # read in data\n",
    "                with rasterio.open(targetSplits + sample) as samp_src:\n",
    "                    samp_data = samp_src.read(1)\n",
    "                    meta = samp_src.meta.copy()\n",
    "                    samp_extent = samp_src.bounds\n",
    "                    samp_transform = samp_src.transform\n",
    "                    samp_crs = samp_src.crs\n",
    "                    # apply a no-data mask\n",
    "                    mask = samp_data >= 0\n",
    "                    msked_target = np.where(mask, samp_data, -1)\n",
    "                    target_shape = msked_target.shape\n",
    "        \n",
    "                    # flatted data\n",
    "                    samp_flat = msked_target.flatten()\n",
    "                    \n",
    "    \n",
    "                # try to get the fsca variables \n",
    "                sample_root = \"_\".join(sample.split(\"_\")[:2])\n",
    "                for fSCA in os.listdir(fSCAWorkspace):\n",
    "                    if fSCA.endswith(extension_filter) and fSCA.startswith(sample_root):\n",
    "                        featureName.append(f\"{fSCA[:-4]}\")\n",
    "                        fsca_norm = read_aligned_raster(src_path=fSCAWorkspace + fSCA, extent=samp_extent, target_shape=target_shape)\n",
    "                        fsca_norm = min_max_scale(fsca_norm, min_val=0, max_val=100)\n",
    "                        featureTuple += (fsca_norm,)\n",
    "                        # print(fsca_norm.shape)\n",
    "                        if fsca_norm.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: FSCA\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_FSCA.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "        \n",
    "                # get a DOY array into a feature \n",
    "                date_string = sample.split(\"_\")[1]\n",
    "                doy_str = date_string[-3:]\n",
    "                doy = float(doy_str)\n",
    "                DOY_array = np.full_like(msked_target, doy)\n",
    "                doy_norm = min_max_scale(DOY_array,  min_val=0, max_val=366)\n",
    "                featureTuple += (doy_norm,)\n",
    "                featureName.append(doy)\n",
    "        \n",
    "                # get the vegetation array\n",
    "                for tree in os.listdir(vegetation_path):\n",
    "                    if tree.endswith(extension_filter):\n",
    "                        if tree.startswith(f\"{year}\"):\n",
    "                            featureName.append(f\"{tree[:-4]}\")\n",
    "                            tree_norm = read_aligned_raster(\n",
    "                            src_path=tree_workspace + tree,\n",
    "                            extent=samp_extent,\n",
    "                            target_shape=target_shape\n",
    "                            )\n",
    "                            tree_norm = min_max_scale(tree_norm, min_val=0, max_val=100)\n",
    "                            featureTuple += (tree_norm,)\n",
    "                            if tree_norm.shape != desired_shape:\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: TREE\")\n",
    "                                output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_TREE.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                # get the vegetation array\n",
    "                for land in os.listdir(land_workspace):\n",
    "                    if land.endswith(\".tif\"):\n",
    "                        if land.startswith(f\"{year}\"):\n",
    "                            # featureName.append(f\"{tree[:-4]}\")\n",
    "                            featureName.append(f\"LandCover\")\n",
    "                            land_norm = read_aligned_raster(\n",
    "                            src_path=land_workspace + land,\n",
    "                            extent=samp_extent,\n",
    "                            target_shape=target_shape\n",
    "                            )\n",
    "                            land_norm = min_max_scale(land_norm, min_val=11, max_val=95)\n",
    "                            featureTuple += (land_norm,)\n",
    "                            if land_norm.shape != (256, 256):\n",
    "                                print(f\"WRONG SHAPE FOR {sample}: Land\")\n",
    "                                # output_debug_path = f\"./debug_output/{sample_root}_BAD_TREE.tif\"\n",
    "                                save_array_as_raster(\n",
    "                                    output_path=output_debug_path,\n",
    "                                    array=fsca_norm,\n",
    "                                    extent=samp_extent,\n",
    "                                    crs=samp_crs,\n",
    "                                    nodata_val=-1\n",
    "                                )\n",
    "                \n",
    "                # # get all the features in the fodler \n",
    "                for phv in os.listdir(phv_path):\n",
    "                    if phv.endswith(extension_filter):\n",
    "                        featureName.append(f\"{phv[:-4]}\")\n",
    "                        phv_data = read_aligned_raster(src_path=phv_features + phv, extent=samp_extent, target_shape=target_shape)\n",
    "                        featureTuple += (phv_data,)\n",
    "                        if phv_data.shape != desired_shape:\n",
    "                            print(f\"WRONG SHAPE FOR {sample}: {phv}\")\n",
    "                            output_debug_path = debug_output_folder + f\"/{sample_root}_BAD_{phv[:-4]}.tif\"\n",
    "                            save_array_as_raster(\n",
    "                                output_path=output_debug_path,\n",
    "                                array=fsca_norm,\n",
    "                                extent=samp_extent,\n",
    "                                crs=samp_crs,\n",
    "                                nodata_val=-1\n",
    "                            )\n",
    "                feature_stack = np.dstack(featureTuple)\n",
    "                if feature_stack.shape[2] != num_of_channels:\n",
    "                    print(f\"{sample} has shape {feature_stack.shape} — missing or extra feature?\")\n",
    "                    print(featureName)\n",
    "                    print(\" \")\n",
    "                else:\n",
    "                    featureArray.append(feature_stack)\n",
    "                    targetArray.append(samp_flat)\n",
    "                    extent_list.append(samp_extent)\n",
    "                    crs_list.append(samp_crs)\n",
    "        # print(\"You go girl!\")\n",
    "        # print(\"all data split into target and feature array\")\n",
    "        return  np.array(featureArray), np.array(targetArray), extent_list, crs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac44b85-6468-41d3-a8ab-f64129c3fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"############### MOVING TO TEST GROUPS OF DATA. FINGERS CROSSED! ###############\")\n",
    "print(\"Testing Group 1\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup1 = inter_model_outWorkspace + f\"outTifs_G1_yPreds_tifs/\"\n",
    "X_train_g1, y_train_g1, g1_train_extents, g1_train_crs = target_feature_stacks_testGroups(year=2022, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group1/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                                   landCover_path = land_workspace,\n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 18)\n",
    "X_test_g1, y_test_g1, g1_test_extents, g1_test_crs = target_feature_stacks_testGroups(year = 2022,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group1/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2022/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\",\n",
    "                                               landCover_path = land_workspace,                                       \n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 18)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g1 = np.array(X_test_g1)\n",
    "y_test_array_g1 = np.array(y_test_g1)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g1.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g1.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g1 = model.predict(X_test_array_g1, batch_size=32)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g1, y_test_array_g1, batch_size=32)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g1):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g1[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup1}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g1_test_extents[i],\n",
    "        crs=g1_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g1, y_test_array_g1, batch_size=32, return_dict=True)\n",
    "print(\"\\n Test Group 1 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef485842-458d-4759-bc12-19eb6398356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"\\n Testing Group 2\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G2_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup2 = inter_model_outWorkspace + f\"outTifs_G2_yPreds_tifs/\"\n",
    "\n",
    "X_train_g2, y_train_g2, g2_train_extents, g2_train_crs = target_feature_stacks_testGroups(year=2023, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group2/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2023/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",\n",
    "                                                   landCover_path = land_workspace,\n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 18)\n",
    "X_test_g2, y_test_g2, g2_test_extents, g2_test_crs = target_feature_stacks_testGroups(year = 2023,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group2/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2023/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               landCover_path = land_workspace,\n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 18)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g2 = np.array(X_test_g2)\n",
    "y_test_array_g2 = np.array(y_test_g2)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g2.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g2.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g2 = model.predict(X_test_array_g2, batch_size=32)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g2, y_test_array_g2, batch_size=32)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g2):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g2[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup2}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g2_test_extents[i],\n",
    "        crs=g2_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g2, y_test_array_g2, batch_size=32, return_dict=True)\n",
    "print(\"\\n Test Group 2 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973c25e-1351-4229-99b9-b37c404855d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder\n",
    "print(\"\\n Testing Group 3\")\n",
    "os.makedirs(inter_model_outWorkspace + f\"outTifs_G3_yPreds_tifs/\", exist_ok=True)\n",
    "outGroup3 = inter_model_outWorkspace + f\"outTifs_G3_yPreds_tifs/\"\n",
    "\n",
    "X_train_g3, y_train_g3, g3_train_extents, g3_train_crs = target_feature_stacks_testGroups(year=2025, \n",
    "                                                   target_splits_path = WorkspaceBase + f\"test_groups/Group3/train/\", \n",
    "                                                   fSCA_path = WorkspaceBase + f\"2025/fSCA/\", \n",
    "                                                   vegetation_path = WorkspaceBase + \"treeCover/\",  \n",
    "                                                   landCover_path = land_workspace,\n",
    "                                                   phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                                   extension_filter = \".tif\", \n",
    "                                                   desired_shape = (256, 256), \n",
    "                                                   debug_output_folder = \"./debug_outputs/\", \n",
    "                                                   num_of_channels = 18)\n",
    "X_test_g3, y_test_g3, g3_test_extents, g3_test_crs = target_feature_stacks_testGroups(year = 2025,\n",
    "                                               target_splits_path = WorkspaceBase + f\"test_groups/Group3/test/\", \n",
    "                                               fSCA_path = WorkspaceBase + f\"2025/fSCA/\", \n",
    "                                               vegetation_path = WorkspaceBase + \"treeCover/\", \n",
    "                                               landCover_path = land_workspace,\n",
    "                                               phv_path = WorkspaceBase + \"features/scaled/\", \n",
    "                                               extension_filter = \".tif\", \n",
    "                                               desired_shape = (256, 256), \n",
    "                                               debug_output_folder = \"./debug_outputs/\", \n",
    "                                               num_of_channels = 18)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Custom loss function must be re-declared if you used one\n",
    "model = load_model(f\"{inter_model_outWorkspace}/best_model_{timestamp}.keras\", custom_objects={\n",
    "    'loss': masked_rmse\n",
    "})\n",
    "\n",
    "\n",
    "# Convert lists to NumPy arrays if they aren't already\n",
    "X_test_array_g3 = np.array(X_test_g3)\n",
    "y_test_array_g3 = np.array(y_test_g3)\n",
    "\n",
    "# Make sure shapes are correct for model input\n",
    "print(f\"X_test shape: {X_test_array_g3.shape}\")  # should be (num_samples, 256, 256, num_channels)\n",
    "print(f\"y_test shape: {y_test_array_g3.shape}\")  # should be (num_samples, 256, 256, 1) or (num_samples, 256, 256)\n",
    "\n",
    "# model predictions\n",
    "y_pred_g3 = model.predict(X_test_array_g3, batch_size=16)\n",
    "\n",
    "# test loss\n",
    "loss = model.evaluate(X_test_array_g3, y_test_array_g3, batch_size=16)\n",
    "\n",
    "for i, pred in enumerate(y_pred_g3):\n",
    "    array = pred.reshape((256, 256))\n",
    "    mask = y_test_g3[i].reshape((256, 256)) != -1  # True where data is valid\n",
    "    \n",
    "    # Apply mask: set prediction to -1 where original target had no data\n",
    "    array_masked = np.where(mask, array, -1)\n",
    "    save_array_as_raster(\n",
    "        output_path=f\"{outGroup3}/prediction_{i}.tif\",\n",
    "        array=array_masked.astype(np.float32),\n",
    "        extent=g3_test_extents[i],\n",
    "        crs=g3_test_crs[i],\n",
    "        nodata_val=-1\n",
    "    )\n",
    "\n",
    "# Print test loss and metrics\n",
    "metrics = model.evaluate(X_test_array_g3, y_test_array_g3, batch_size=32, return_dict=True)\n",
    "\n",
    "print(\"\\n Test Group 3 Metrics:\")\n",
    "print(f\"Masked MSE (Loss): {metrics['loss']:.4f}\")\n",
    "print(f\"Masked RMSE:       {metrics['masked_rmse']:.4f}\")\n",
    "print(f\"Masked MAE:        {metrics['masked_mae']:.4f}\")\n",
    "print(f\"Masked MSE Metric: {metrics['masked_mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c9e354-43d8-49a5-8efe-691ae5f798b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_seconds = end_time - start_time\n",
    "total_minutes = total_seconds / 60\n",
    "print(\"\\n Model Completed\")\n",
    "print(f\"Training time: {total_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acf979-2408-4929-b9f2-e41fe9563c23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aso-dl)",
   "language": "python",
   "name": "aso-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
